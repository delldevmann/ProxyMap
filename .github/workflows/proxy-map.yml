name: Generate Proxy Map

on:
  schedule:
    - cron: '0 */6 * * *'  # Run every 6 hours
  workflow_dispatch:  # Allow manual triggering

permissions:
  contents: write
  pages: write
  id-token: write

jobs:
  build-and-deploy:
    runs-on: ubuntu-22.04
    
    environment:
      name: github-pages
      url: ${{ steps.deployment.outputs.page_url }}
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'

      - name: Install dependencies
        run: pip install folium requests matplotlib pandas

      - name: Generate proxy map
        run: |
          python3 << 'EOF'
          import requests, folium, pandas as pd, matplotlib.pyplot as plt
          from folium.plugins import Fullscreen, MiniMap, FloatImage
          import os

          # Fetch proxy data from multiple sources - ALWAYS use both
          import re
          from datetime import datetime
          
          def fetch_latest_all_proxies_url():
              """Find the latest all_proxies_*.json file"""
              try:
                  # Get the directory listing from GitHub API
                  api_url = "https://api.github.com/repos/delldevmann/proxy-scraper/contents/results"
                  response = requests.get(api_url, timeout=30)
                  
                  if response.status_code != 200:
                      print(f"GitHub API returned status {response.status_code}")
                      return None
                      
                  files = response.json()
                  print(f"Found {len(files)} files in results directory")
                  
                  # Find all_proxies files and get the latest one
                  all_proxies_files = []
                  for file in files:
                      filename = file['name']
                      if filename.startswith('all_proxies_') and filename.endswith('.json'):
                          print(f"Found all_proxies file: {filename}")
                          # Extract timestamp from filename
                          match = re.search(r'all_proxies_(\d{8}_\d{6})\.json', filename)
                          if match:
                              try:
                                  timestamp = datetime.strptime(match.group(1), '%Y%m%d_%H%M%S')
                                  all_proxies_files.append((timestamp, file['download_url'], filename))
                              except ValueError as e:
                                  print(f"Could not parse timestamp from {filename}: {e}")
                  
                  if all_proxies_files:
                      # Sort by timestamp and get the latest
                      all_proxies_files.sort(key=lambda x: x[0], reverse=True)
                      latest_file = all_proxies_files[0]
                      print(f"Latest all_proxies file: {latest_file[2]} ({latest_file[0]})")
                      return latest_file[1], latest_file[2]
                  else:
                      print("No all_proxies files found with valid timestamps")
                      return None, None
                      
              except Exception as e:
                  print(f"Error finding latest all_proxies file: {e}")
                  import traceback
                  traceback.print_exc()
                  return None, None

          # Data sources - ALWAYS fetch both
          summary_url = 'https://raw.githubusercontent.com/delldevmann/proxy-scraper/main/results/summary_latest.json'
          all_proxies_url, all_proxies_filename = fetch_latest_all_proxies_url()
          
          proxies = []
          data_sources = []
          
          # MANDATORY: Fetch summary data first (sample proxies with rich metadata)
          print("=== FETCHING SUMMARY DATA (REQUIRED) ===")
          try:
              print(f"Fetching summary data from: {summary_url}")
              summary_response = requests.get(summary_url, timeout=30)
              if summary_response.status_code == 200:
                  summary_data = summary_response.json()
                  print(f"Summary data type: {type(summary_data)}")
                  print(f"Summary data keys: {list(summary_data.keys()) if isinstance(summary_data, dict) else 'Not a dict'}")
                  
                  summary_count = 0
                  for group_name, group in summary_data.items():
                      sample_proxies = group.get("sample_proxies", {})
                      print(f"Group '{group_name}' has {len(sample_proxies)} sample proxies")
                      
                      for proxy in sample_proxies.values():
                          loc = proxy.get("location", {})
                          if loc.get("lat") and loc.get("lon"):
                              proxies.append({
                                  "ip": proxy["ip"],
                                  "port": proxy.get("port", "Unknown"),
                                  "latency": proxy.get("latency_ms", 9999),
                                  "anonymity": proxy.get("anonymity", "Unknown"),
                                  "lat": loc["lat"],
                                  "lon": loc["lon"],
                                  "city": loc.get("city", "Unknown"),
                                  "country": loc.get("country", "Unknown"),
                                  "code": loc.get("countryCode", "xx"),
                                  "isp": loc.get("isp", "N/A"),
                                  "source": "summary",
                                  "protocol": proxy.get("protocol", "HTTP")
                              })
                              summary_count += 1
                  
                  data_sources.append(f"Summary: {summary_count} proxies")
                  print(f"✅ Successfully loaded {summary_count} proxies from summary")
              else:
                  print(f"❌ Failed to fetch summary data: HTTP {summary_response.status_code}")
                  
          except Exception as e:
              print(f"❌ Error fetching summary data: {e}")
              import traceback
              traceback.print_exc()

          # MANDATORY: Fetch complete dataset 
          print(f"\n=== FETCHING COMPLETE DATASET (REQUIRED) ===")
          if all_proxies_url:
              try:
                  print(f"Fetching complete dataset from: {all_proxies_filename}")
                  print(f"URL: {all_proxies_url}")
                  
                  all_proxies_response = requests.get(all_proxies_url, timeout=120)
                  if all_proxies_response.status_code == 200:
                      all_proxies_data = all_proxies_response.json()
                      
                      print(f"Complete dataset type: {type(all_proxies_data)}")
                      if isinstance(all_proxies_data, (list, dict)):
                          print(f"Complete dataset size: {len(all_proxies_data)}")
                      
                      # Handle different data structures
                      proxy_list = []
                      if isinstance(all_proxies_data, list):
                          proxy_list = all_proxies_data
                          print("✅ Data is a simple list")
                      elif isinstance(all_proxies_data, dict):
                          print("🔍 Data is a dictionary, extracting proxies...")
                          for key, value in all_proxies_data.items():
                              print(f"  Key '{key}': {type(value)} with {len(value) if isinstance(value, (list, dict)) else 'unknown'} items")
                              if isinstance(value, list):
                                  proxy_list.extend(value)
                              elif isinstance(value, dict):
                                  if 'proxies' in value:
                                      proxy_list.extend(value['proxies'])
                                  elif 'sample_proxies' in value:
                                      proxy_list.extend(value['sample_proxies'].values())
                                  else:
                                      # Assume the dict values are proxy objects
                                      proxy_list.extend(value.values())
                      
                      print(f"✅ Extracted {len(proxy_list)} total proxies from complete dataset")
                      
                      # Debug: Show a sample of what we extracted
                      if len(proxy_list) > 0:
                          print(f"🔍 Sample of extracted data:")
                          for j, sample in enumerate(proxy_list[:3]):
                              print(f"  Item {j}: {type(sample)} - {sample if not isinstance(sample, dict) else f'dict with keys: {list(sample.keys())[:5]}'}")
                      
                      
                      # Process complete dataset
                      initial_count = len(proxies)
                      existing_ips = {p["ip"] for p in proxies}
                      processed_count = 0
                      added_count = 0
                      
                      for i, proxy in enumerate(proxy_list):
                          if i % 2000 == 0 and i > 0:
                              print(f"  Processing: {i}/{len(proxy_list)} ({added_count} added so far)")
                          
                          # Handle different proxy formats
                          proxy_ip = proxy.get("ip") or proxy.get("host") or proxy.get("address")
                          if not proxy_ip:
                              continue
                              
                          if proxy_ip in existing_ips:
                              continue
                          
                          # Try different location field names
                          loc = (proxy.get("location") or 
                                 proxy.get("geo") or 
                                 proxy.get("geolocation") or 
                                 {})
                          
                          # Try different lat/lon field names
                          lat = (loc.get("lat") or loc.get("latitude") or 
                                 proxy.get("lat") or proxy.get("latitude"))
                          lon = (loc.get("lon") or loc.get("lng") or loc.get("longitude") or 
                                 proxy.get("lon") or proxy.get("lng") or proxy.get("longitude"))
                          
                          if lat is not None and lon is not None:
                              try:
                                  # Ensure lat/lon are numbers
                                  lat = float(lat)
                                  lon = float(lon)
                                  
                                  # Validate coordinates
                                  if -90 <= lat <= 90 and -180 <= lon <= 180:
                                      # Get latency from various possible fields
                                      latency = (proxy.get("latency_ms") or 
                                               proxy.get("response_time_ms") or 
                                               proxy.get("latency") or 
                                               proxy.get("response_time") or 
                                               9999)
                                      
                                      if isinstance(latency, str):
                                          try:
                                              latency = float(latency.replace('ms', '').strip())
                                          except:
                                              latency = 9999
                                      
                                      proxies.append({
                                          "ip": proxy_ip,
                                          "port": proxy.get("port", "Unknown"),
                                          "latency": float(latency) if latency != 9999 else 9999,
                                          "anonymity": proxy.get("anonymity", "Unknown"),
                                          "lat": lat,
                                          "lon": lon,
                                          "city": loc.get("city", "Unknown"),
                                          "country": loc.get("country", "Unknown"),
                                          "code": loc.get("countryCode", "xx"),
                                          "isp": loc.get("isp", "N/A"),
                                          "source": "complete",
                                          "protocol": proxy.get("protocol", "HTTP")
                                      })
                                      existing_ips.add(proxy_ip)
                                      added_count += 1
                                      
                              except (ValueError, TypeError):
                                  continue
                          
                          processed_count += 1
                      
                      data_sources.append(f"Complete dataset ({all_proxies_filename}): +{added_count} additional proxies")
                      print(f"✅ Successfully added {added_count} additional proxies from complete dataset")
                      print(f"📊 Processed {processed_count} total records, {added_count} had valid coordinates")
                  else:
                      print(f"❌ Failed to fetch complete dataset: HTTP {all_proxies_response.status_code}")
                      
              except Exception as e:
                  print(f"❌ Error fetching complete dataset: {e}")
                  import traceback
                  traceback.print_exc()
          else:
              print("❌ Could not find latest all_proxies file - this is REQUIRED!")
              # Don't exit, but note the issue
              data_sources.append("⚠️  Complete dataset: Not found")

          print(f"\n=== FINAL RESULTS ===")
          print(f"Total proxies loaded: {len(proxies)}")
          for source in data_sources:
              print(f"  - {source}")

          if not proxies:
              print("No proxy data found")
              exit(1)

          df = pd.DataFrame(proxies)
          lat, lon = df["lat"].mean(), df["lon"].mean()

          # Create the map
          m = folium.Map(
              location=[lat, lon], 
              zoom_start=2, 
              tiles="CartoDB positron", 
              control_scale=True
          )
          
          # Add plugins
          Fullscreen().add_to(m)
          MiniMap(toggle_display=True).add_to(m)

          # Add markers for each proxy with enhanced info
          for _, row in df.iterrows():
              color = 'green' if row["latency"] < 1000 else 'orange' if row["latency"] < 2000 else 'red'
              flag = f"https://flagcdn.com/24x18/{row['code'].lower()}.png"
              
              # Enhanced popup with more details
              popup = f"""
              <div style='font-family:Arial; font-size:13px; width:280px'>
                <img src='{flag}' style='vertical-align:middle' onerror="this.style.display='none'"> 
                <b>{row['country']}</b><br>
                <hr style='margin: 5px 0;'>
                <b>🌐 IP:</b> {row['ip']}:{row.get('port', 'N/A')}<br>
                <b>📍 City:</b> {row['city']}<br>
                <b>🏢 ISP:</b> {row['isp']}<br>
                <b>⚡ Latency:</b> {row['latency']} ms<br>
                <b>🔒 Anonymity:</b> {row['anonymity']}<br>
                <b>📡 Protocol:</b> {row.get('protocol', 'HTTP')}<br>
                <b>📊 Source:</b> {row.get('source', 'summary').title()}
              </div>
              """
              
              # Different marker styles based on source
              marker_size = 8 if row.get('source') == 'complete' else 6
              opacity = 0.8 if row.get('source') == 'complete' else 0.9
              
              folium.CircleMarker(
                  location=[row["lat"], row["lon"]],
                  radius=marker_size,
                  color=color,
                  fill=True,
                  fill_opacity=opacity,
                  popup=folium.Popup(popup, max_width=320),
                  tooltip=f"{row['ip']} ({row['country']}) - {row.get('source', 'summary').title()}"
              ).add_to(m)

          # Create country distribution chart with better styling
          top_countries = df["country"].value_counts().nlargest(8)
          plt.figure(figsize=(8, 5))
          plt.style.use('seaborn-v0_8')
          
          colors = plt.cm.Set3(range(len(top_countries)))
          bars = plt.barh(range(len(top_countries)), top_countries.values, color=colors)
          
          plt.yticks(range(len(top_countries)), top_countries.index)
          plt.xlabel("Number of Proxies", fontsize=11, fontweight='bold')
          plt.title("Top Countries by Proxy Count", fontsize=12, fontweight='bold', pad=15)
          plt.grid(axis='x', alpha=0.3, linestyle='--')
          
          # Add value labels on bars
          for i, (country, count) in enumerate(top_countries.items()):
              plt.text(count + 0.5, i, str(count), va='center', ha='left', fontweight='bold', fontsize=9)
          
          # Style improvements
          plt.gca().spines['top'].set_visible(False)
          plt.gca().spines['right'].set_visible(False)
          plt.gca().spines['left'].set_visible(False)
          plt.tight_layout()

          # Create output directory
          os.makedirs("public", exist_ok=True)
          
          # Save chart with white background
          plt.savefig("public/country_chart.png", dpi=120, bbox_inches='tight', 
                     facecolor='white', edgecolor='none')
          plt.close()

          # Add enhanced stats info box
          stats_html = f"""
          <div style="position: fixed; 
                      top: 10px; left: 10px; width: 220px; 
                      background-color: rgba(255,255,255,0.95); 
                      border: 2px solid #333; border-radius: 8px;
                      z-index: 9999; font-size: 12px; padding: 12px;
                      box-shadow: 0 2px 10px rgba(0,0,0,0.2);">
          <h4 style="margin: 0 0 8px 0; color: #333;">📊 Proxy Statistics</h4>
          <div style="color: #555;">
            <strong>Total Proxies:</strong> {len(proxies)}<br>
            <strong>Countries:</strong> {df['country'].nunique()}<br>
            <strong>Avg Latency:</strong> {df['latency'].mean():.0f}ms<br>
            <strong>Data Sources:</strong> {len(data_sources)}<br>
            <strong>Last Update:</strong> Now
          </div>
          <hr style="margin: 8px 0;">
          <div style="font-size: 10px; color: #777;">
            {'<br>'.join(data_sources)}
          </div>
          </div>
          """
          
          # Add improved legend
          legend_html = """
          <div style="position: fixed; 
                      top: 10px; right: 10px; width: 140px; 
                      background-color: rgba(255,255,255,0.95); 
                      border: 2px solid #333; border-radius: 8px;
                      z-index: 9999; font-size: 12px; padding: 12px;
                      box-shadow: 0 2px 10px rgba(0,0,0,0.2);">
          <h4 style="margin: 0 0 8px 0; color: #333;">🚦 Latency</h4>
          <div style="display: flex; align-items: center; margin: 4px 0;">
            <div style="width: 12px; height: 12px; background-color: green; border-radius: 50%; margin-right: 8px;"></div>
            <span style="color: #555;">< 1000ms</span>
          </div>
          <div style="display: flex; align-items: center; margin: 4px 0;">
            <div style="width: 12px; height: 12px; background-color: orange; border-radius: 50%; margin-right: 8px;"></div>
            <span style="color: #555;">1000-2000ms</span>
          </div>
          <div style="display: flex; align-items: center; margin: 4px 0;">
            <div style="width: 12px; height: 12px; background-color: red; border-radius: 50%; margin-right: 8px;"></div>
            <span style="color: #555;">> 2000ms</span>
          </div>
          </div>
          """
          
          # Add chart as floating image - positioned better
          FloatImage("public/country_chart.png", bottom=15, right=15).add_to(m)
          
          # Add both info boxes
          m.get_root().html.add_child(folium.Element(stats_html))
          m.get_root().html.add_child(folium.Element(legend_html))
          
          # Save map
          m.save("public/index.html")
          
          print(f"Generated map with {len(proxies)} proxies from {df['country'].nunique()} countries")
          EOF

      - name: Upload Pages artifact
        uses: actions/upload-pages-artifact@v3
        with:
          path: public

  deploy:
    needs: build-and-deploy
    runs-on: ubuntu-22.04
    
    environment:
      name: github-pages
      url: ${{ steps.deployment.outputs.page_url }}
    
    permissions:
      pages: write
      id-token: write
    
    steps:
      - name: Deploy to GitHub Pages
        id: deployment
        uses: actions/deploy-pages@v4
