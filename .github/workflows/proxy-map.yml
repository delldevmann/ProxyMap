name: Generate Enhanced Proxy Map - All Sources

on:
  schedule:
    - cron: '0 */6 * * *'  # Run every 6 hours
  workflow_dispatch:  # Allow manual triggering

permissions:
  contents: write
  pages: write
  id-token: write

jobs:
  build-and-deploy:
    runs-on: ubuntu-22.04
    
    environment:
      name: github-pages
      url: ${{ steps.deployment.outputs.page_url }}
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          pip install --upgrade pip
          pip install folium requests matplotlib pandas seaborn

      - name: Generate proxy map from ALL sources
        run: |
          python3 -c "
import requests, folium, pandas as pd, matplotlib.pyplot as plt
from folium.plugins import Fullscreen, MiniMap, MarkerCluster, MousePosition
import os, re, json
from datetime import datetime
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
import random
import base64
from io import BytesIO

def fetch_with_retry(url, max_retries=3, timeout=60):
    \"\"\"Fetch URL with retry logic\"\"\"
    for attempt in range(max_retries):
        try:
            print(f'  Attempt {attempt + 1}/{max_retries} for {url}')
            response = requests.get(url, timeout=timeout, headers={
                'User-Agent': 'ProxyMapBot/1.0',
                'Accept': 'application/json'
            })
            
            if response.status_code == 200:
                return response.json()
            elif response.status_code == 429:
                wait_time = 2 ** attempt
                print(f'  Rate limited, waiting {wait_time}s...')
                time.sleep(wait_time)
            else:
                print(f'  HTTP {response.status_code}')
                
        except Exception as e:
            print(f'  Error on attempt {attempt + 1}: {e}')
        
        if attempt < max_retries - 1:
            time.sleep(2 ** attempt)
    
    return None

def find_all_proxy_files():
    \"\"\"Find ALL proxy files from the GitHub repo\"\"\"
    try:
        api_url = 'https://api.github.com/repos/delldevmann/proxy-scraper/contents/results'
        response = requests.get(api_url, timeout=30)
        
        if response.status_code != 200:
            print(f'GitHub API returned status {response.status_code}')
            return []
            
        files = response.json()
        print(f'Found {len(files)} files in results directory')
        
        proxy_files = []
        for file in files:
            filename = file['name']
            # Include ALL JSON files that might contain proxies
            if filename.endswith('.json'):
                print(f'Found proxy file: {filename}')
                proxy_files.append((file['download_url'], filename))
        
        # Sort files to prioritize newer dates
        def extract_date(filename):
            # Try to extract date from filename (e.g., all_proxies_20250301_013951.json)
            match = re.search(r'(\d{8})', filename)
            return match.group(1) if match else '00000000'
        
        proxy_files.sort(key=lambda x: extract_date(x[1]), reverse=True)
        
        print(f'Total proxy files found: {len(proxy_files)}')
        return proxy_files
            
    except Exception as e:
        print(f'Error finding proxy files: {e}')
        return []

def process_proxy_data(file_data, filename):
    \"\"\"Universal proxy data processor that handles ANY format\"\"\"
    proxies = []
    
    if not file_data:
        return proxies
    
    print(f'Processing {filename}: {type(file_data)}')
    
    # Method 1: New schema with \"proxies\" key containing protocols
    if isinstance(file_data, dict) and 'proxies' in file_data:
        print(f'  Found new schema format with \\'proxies\\' key')
        proxies_by_type = file_data.get('proxies', {})
        
        for protocol_type, protocol_proxies in proxies_by_type.items():
            if not isinstance(protocol_proxies, dict):
                continue
                
            print(f'  Processing {protocol_type} proxies: {len(protocol_proxies)} entries')
            
            for proxy_key, proxy_data in protocol_proxies.items():
                try:
                    location = proxy_data.get('location', {})
                    lat = location.get('lat')
                    lon = location.get('lon')
                    
                    if lat is not None and lon is not None:
                        lat, lon = float(lat), float(lon)
                        
                        if -90 <= lat <= 90 and -180 <= lon <= 180:
                            port = proxy_key.split(':')[1] if ':' in proxy_key else 'Unknown'
                            
                            proxies.append({
                                'ip': proxy_data.get('ip', proxy_key.split(':')[0] if ':' in proxy_key else proxy_key),
                                'port': port,
                                'latency': proxy_data.get('latency_ms', 9999),
                                'anonymity': proxy_data.get('anonymity', 'Unknown'),
                                'lat': lat,
                                'lon': lon,
                                'city': location.get('city', 'Unknown'),
                                'country': location.get('country', 'Unknown'),
                                'code': location.get('countryCode', 'xx').lower(),
                                'isp': location.get('isp', 'N/A'),
                                'source': filename,
                                'protocol': proxy_data.get('type', protocol_type),
                                'last_checked': proxy_data.get('last_checked', 'Unknown')
                            })
                except Exception as e:
                    continue
        
        return proxies
    
    # Method 2: Direct IP:PORT mapping at root level
    if isinstance(file_data, dict):
        # Check if we have IP:PORT pattern keys
        ip_port_entries = [k for k in file_data.keys() if ':' in k and '.' in k]
        if len(ip_port_entries) > 10:  # Likely a proxy file
            print(f'  Found IP:PORT pattern in keys ({len(ip_port_entries)} entries)')
            
            for ip_port, proxy_info in file_data.items():
                if ':' not in ip_port or not isinstance(proxy_info, dict):
                    continue
                
                try:
                    ip, port = ip_port.split(':', 1)
                    
                    if 'location' in proxy_info:
                        location = proxy_info['location']
                        lat = location.get('lat')
                        lon = location.get('lon')
                        
                        if lat is not None and lon is not None:
                            lat, lon = float(lat), float(lon)
                            
                            if -90 <= lat <= 90 and -180 <= lon <= 180:
                                proxies.append({
                                    'ip': ip,
                                    'port': port,
                                    'latency': proxy_info.get('latency_ms', proxy_info.get('latency', 9999)),
                                    'anonymity': proxy_info.get('anonymity', 'Unknown'),
                                    'lat': lat,
                                    'lon': lon,
                                    'city': location.get('city', 'Unknown'),
                                    'country': location.get('country', 'Unknown'),
                                    'code': location.get('countryCode', 'xx').lower(),
                                    'isp': location.get('isp', 'N/A'),
                                    'source': filename,
                                    'protocol': proxy_info.get('type', proxy_info.get('protocol', 'http')),
                                    'last_checked': proxy_info.get('last_checked', 'Unknown')
                                })
                except Exception:
                    continue
            
            return proxies
    
    # Method 3: List of proxy objects
    proxy_candidates = []
    
    if isinstance(file_data, list):
        proxy_candidates = file_data
    elif isinstance(file_data, dict):
        # Look for common patterns
        for key in ['proxies', 'data', 'results', 'items']:
            if key in file_data and isinstance(file_data[key], list):
                proxy_candidates = file_data[key]
                break
        
        # Check protocol-specific keys
        if not proxy_candidates:
            for key in ['http', 'https', 'socks4', 'socks5']:
                if key in file_data:
                    if isinstance(file_data[key], list):
                        proxy_candidates.extend(file_data[key])
                    elif isinstance(file_data[key], dict):
                        # Could be nested structure
                        sub_data = file_data[key]
                        if 'proxies' in sub_data and isinstance(sub_data['proxies'], list):
                            proxy_candidates.extend(sub_data['proxies'])
    
    print(f'  Found {len(proxy_candidates)} proxy candidates')
    
    # Process candidates
    for proxy in proxy_candidates:
        if not isinstance(proxy, dict):
            continue
        
        try:
            # Extract IP
            ip = None
            for field in ['ip', 'host', 'address', 'addr', 'proxy', 'server']:
                if field in proxy:
                    ip = proxy[field]
                    if ip and ':' in ip:
                        ip = ip.split(':')[0]
                    break
            
            if not ip:
                continue
            
            # Extract location
            lat, lon = None, None
            location = {}
            
            if 'location' in proxy and isinstance(proxy['location'], dict):
                location = proxy['location']
                lat = location.get('lat', location.get('latitude'))
                lon = location.get('lon', location.get('longitude'))
            elif 'geo' in proxy and isinstance(proxy['geo'], dict):
                location = proxy['geo']
                lat = location.get('lat', location.get('latitude'))
                lon = location.get('lon', location.get('longitude'))
            elif 'lat' in proxy and 'lon' in proxy:
                lat = proxy['lat']
                lon = proxy['lon']
                location = proxy
            elif 'latitude' in proxy and 'longitude' in proxy:
                lat = proxy['latitude']
                lon = proxy['longitude']
                location = proxy
            
            if lat is None or lon is None:
                continue
            
            lat, lon = float(lat), float(lon)
            
            if -90 <= lat <= 90 and -180 <= lon <= 180:
                # Extract port
                port = proxy.get('port', proxy.get('Port', 'Unknown'))
                if port == 'Unknown' and ':' in str(proxy.get('ip', '')):
                    port = proxy['ip'].split(':')[1]
                
                # Get country info
                country = location.get('country', proxy.get('country', 'Unknown'))
                country_code = location.get('countryCode', location.get('country_code', proxy.get('country_code', 'xx')))
                
                proxies.append({
                    'ip': ip,
                    'port': str(port),
                    'latency': proxy.get('latency_ms', proxy.get('latency', proxy.get('delay', 9999))),
                    'anonymity': proxy.get('anonymity', proxy.get('anon', 'Unknown')),
                    'lat': lat,
                    'lon': lon,
                    'city': location.get('city', proxy.get('city', 'Unknown')),
                    'country': country,
                    'code': country_code.lower() if country_code else 'xx',
                    'isp': location.get('isp', proxy.get('isp', 'N/A')),
                    'source': filename,
                    'protocol': proxy.get('protocol', proxy.get('type', 'http')),
                    'last_checked': proxy.get('last_checked', proxy.get('checked', 'Unknown'))
                })
        except Exception:
            continue
    
    print(f'✅ Extracted {len(proxies)} valid proxies from {filename}')
    return proxies

# Initialize data collection
all_proxies = []
unique_ips = set()
data_sources = []

print('=== COMPREHENSIVE PROXY DATA COLLECTION ===')
print(f'Starting at: {datetime.now()}')

# Get ALL proxy files
print('\\nFinding ALL proxy files...')
proxy_files = find_all_proxy_files()

if not proxy_files:
    print('❌ No proxy files found!')
    exit(1)

print(f'\\n📊 Will process {len(proxy_files)} proxy files')

# Process files in batches to avoid overwhelming the system
BATCH_SIZE = 5
MAX_FILES = 50  # Increase limit to get more data

files_to_process = proxy_files[:MAX_FILES]
processed_count = 0

for i in range(0, len(files_to_process), BATCH_SIZE):
    batch = files_to_process[i:i + BATCH_SIZE]
    print(f'\\n--- Processing batch {i//BATCH_SIZE + 1}/{(len(files_to_process) + BATCH_SIZE - 1)//BATCH_SIZE} ---')
    
    def fetch_and_process_file(file_info):
        url, filename = file_info
        print(f'\\nFetching {filename}...')
        
        file_data = fetch_with_retry(url, max_retries=2, timeout=90)
        if file_data:
            return process_proxy_data(file_data, filename), filename
        return [], filename
    
    # Process batch with threading
    with ThreadPoolExecutor(max_workers=3) as executor:
        future_to_file = {executor.submit(fetch_and_process_file, file_info): file_info 
                         for file_info in batch}
        
        for future in as_completed(future_to_file):
            try:
                file_proxies, filename = future.result()
                if file_proxies:
                    # Add only unique IPs
                    new_proxies = []
                    for proxy in file_proxies:
                        if proxy['ip'] not in unique_ips:
                            unique_ips.add(proxy['ip'])
                            new_proxies.append(proxy)
                    
                    if new_proxies:
                        all_proxies.extend(new_proxies)
                        data_sources.append(f'{filename}: {len(new_proxies)} unique')
                        print(f'✅ Added {len(new_proxies)} new unique proxies from {filename}')
                    else:
                        print(f'ℹ️  No new unique proxies from {filename}')
                
                processed_count += 1
                
            except Exception as e:
                print(f'❌ Error processing file: {e}')
    
    # Small delay between batches
    if i + BATCH_SIZE < len(files_to_process):
        time.sleep(1)

print(f'\\n=== FINAL RESULTS ===')
print(f'🎉 Total unique proxies collected: {len(all_proxies):,}')
print(f'📊 Successfully processed: {processed_count}/{len(files_to_process)} files')
print(f'📁 Data sources with unique proxies: {len(data_sources)}')

if data_sources:
    print('\\n📋 Sources breakdown:')
    for i, source in enumerate(data_sources[:20], 1):  # Show top 20
        print(f'  {i}. {source}')
    if len(data_sources) > 20:
        print(f'  ... and {len(data_sources) - 20} more sources')

if not all_proxies:
    print('❌ No proxy data collected - cannot generate map')
    exit(1)

# Analyze the data
df = pd.DataFrame(all_proxies)

# Clean up latency data
df['latency'] = pd.to_numeric(df['latency'], errors='coerce').fillna(9999)

print(f'\\n📈 ANALYSIS:')
print(f'  🌍 Countries represented: {df[\"country\"].nunique()}')
print(f'  ⚡ Average latency: {df[df[\"latency\"] < 9999][\"latency\"].mean():.0f}ms')
print(f'  🚀 Fast proxies (<1000ms): {len(df[df[\"latency\"] < 1000]):,}')
print(f'  🟡 Medium proxies (1000-2000ms): {len(df[(df[\"latency\"] >= 1000) & (df[\"latency\"] < 2000)]):,}')
print(f'  🔴 Slow proxies (>2000ms): {len(df[df[\"latency\"] >= 2000]):,}')

# Protocol distribution
protocol_dist = df['protocol'].value_counts()
print(f'\\n  📡 Protocol distribution:')
for protocol, count in protocol_dist.items():
    print(f'    {protocol}: {count:,}')

# Top countries
top_countries = df['country'].value_counts().head(15)
print(f'\\n  🏆 Top 15 countries:')
for country, count in top_countries.items():
    print(f'    {country}: {count:,}')

# Generate the enhanced map
print(f'\\n🗺️ Generating interactive map...')

# Calculate map center
lat_center = df['lat'].median()
lon_center = df['lon'].median()

# Create map with cleaner defaults
m = folium.Map(
    location=[lat_center, lon_center], 
    zoom_start=2, 
    tiles='CartoDB dark_matter',
    control_scale=True,
    prefer_canvas=True,
    zoom_control=False,  # Remove default zoom control
    attribution_control=False  # Remove attribution for cleaner look
)

# Add custom zoom control in a better position
from folium.plugins import Fullscreen, MiniMap, MarkerCluster, MousePosition

# Add fullscreen button (top right)
Fullscreen(
    position='topright',
    title='Fullscreen',
    title_cancel='Exit Fullscreen',
    force_separate_button=True
).add_to(m)

# Add cleaner mini map
minimap = MiniMap(
    toggle_display=True,
    tile_layer='CartoDB dark_matter',
    position='bottomright',
    width=150,
    height=150,
    collapsed_width=25,
    collapsed_height=25,
    zoom_level_offset=-5,
    minimized=True
).add_to(m)

# Add mouse position coordinate display
MousePosition(
    position='bottomright',
    separator=' | ',
    empty_string='',
    lng_first=False,
    num_digits=3,
    prefix='',
    lat_formatter='function(lat){return \\'Lat: \\' + lat.toFixed(3)}',
    lng_formatter='function(lng){return \\'Lng: \\' + lng.toFixed(3)}'
).add_to(m)

# Use marker clustering with cleaner options
marker_cluster = MarkerCluster(
    name='Proxy Locations',
    overlay=True,
    control=False,  # Don't show in layer control
    options={
        'maxClusterRadius': 60,
        'disableClusteringAtZoom': 8,
        'spiderfyOnMaxZoom': True,
        'showCoverageOnHover': False,
        'zoomToBoundsOnClick': True,
        'singleMarkerMode': False,
        'spiderLegPolylineOptions': {'weight': 1.5, 'color': '#fff', 'opacity': 0.5},
        'iconCreateFunction': '''
            function(cluster) {
                var childCount = cluster.getChildCount();
                var c = \\' marker-cluster-\\';
                if (childCount < 10) {
                    c += \\'small\\';
                } else if (childCount < 100) {
                    c += \\'medium\\';
                } else {
                    c += \\'large\\';
                }
                return new L.DivIcon({
                    html: \\'<div><span>\\' + childCount + \\'</span></div>\\',
                    className: \\'marker-cluster\\' + c,
                    iconSize: new L.Point(40, 40)
                });
            }
        '''
    }
).add_to(m)

# Create visualizations
os.makedirs('public', exist_ok=True)

# Enhanced charts with cleaner style
plt.style.use('dark_background')
fig = plt.figure(figsize=(14, 8))
fig.patch.set_facecolor('#0a0a0a')

# Layout: 2x2 grid with adjusted spacing
gs = fig.add_gridspec(2, 2, hspace=0.3, wspace=0.3)
ax1 = fig.add_subplot(gs[0, 0])  # Top countries
ax2 = fig.add_subplot(gs[0, 1])  # Protocol distribution
ax3 = fig.add_subplot(gs[1, :])  # Latency distribution (full width)

# 1. Top countries bar chart - horizontal bars
top_countries_10 = df['country'].value_counts().nlargest(10)
y_pos = range(len(top_countries_10))

bars = ax1.barh(y_pos, top_countries_10.values, color='#00ff88', alpha=0.8)
ax1.set_yticks(y_pos)
ax1.set_yticklabels(top_countries_10.inde
