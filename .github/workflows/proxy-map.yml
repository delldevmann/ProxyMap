name: Complete Proxy Scraper - ALL Sources

on:
  schedule:
    - cron: '0 */6 * * *'  # Run every 6 hours
  workflow_dispatch:  # Allow manual triggering

permissions:
  contents: write
  pages: write
  id-token: write

jobs:
  scrape-all-proxies:
    runs-on: ubuntu-22.04
    timeout-minutes: 60  # Increase timeout for large processing
    
    environment:
      name: github-pages
      url: ${{ steps.deployment.outputs.page_url }}
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          pip install --upgrade pip
          pip install folium requests matplotlib pandas seaborn numpy aiohttp asyncio

      - name: Scrape ALL proxy files
        run: |
          python3 << 'EOF'
          import requests
          import folium
          import pandas as pd
          import matplotlib.pyplot as plt
          import numpy as np
          from folium.plugins import Fullscreen, MiniMap, MarkerCluster, MousePosition, HeatMap
          import os
          import re
          import json
          from datetime import datetime
          import time
          import asyncio
          import aiohttp
          from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed
          from collections import defaultdict
          import gc
          
          # Enhanced configuration for maximum collection
          MAX_CONCURRENT_REQUESTS = 10
          MAX_TOTAL_PROXIES = 100000  # Increased for comprehensive collection
          REQUEST_TIMEOUT = 60
          MAX_RETRIES = 2
          CHUNK_SIZE = 50  # Process in chunks to manage memory
          
          print("üöÄ COMPREHENSIVE PROXY COLLECTION SYSTEM")
          print("=" * 60)
          print(f"üéØ Target: Collect ALL proxies from delldevmann/proxy-scraper")
          print(f"‚è∞ Started: {datetime.now()}")
          print(f"üîß Config: {MAX_CONCURRENT_REQUESTS} concurrent, {MAX_TOTAL_PROXIES:,} max proxies")
          
          def fetch_with_retry(session, url, max_retries=MAX_RETRIES):
              """Enhanced fetch with session reuse"""
              for attempt in range(max_retries):
                  try:
                      response = session.get(url, timeout=REQUEST_TIMEOUT)
                      if response.status_code == 200:
                          try:
                              return response.json()
                          except json.JSONDecodeError:
                              print(f"‚ùå Invalid JSON: {url}")
                              return None
                      elif response.status_code == 429:
                          wait_time = min(2 ** attempt, 30)
                          print(f"‚è∏Ô∏è  Rate limited, waiting {wait_time}s...")
                          time.sleep(wait_time)
                      elif response.status_code == 404:
                          return None
                      else:
                          print(f"‚ö†Ô∏è  HTTP {response.status_code}: {url}")
                  except Exception as e:
                      print(f"‚ùå Error attempt {attempt + 1}: {str(e)[:100]}")
                  
                  if attempt < max_retries - 1:
                      time.sleep(min(2 ** attempt, 10))
              
              return None
          
          def discover_all_files():
              """Discover ALL available proxy files using multiple strategies"""
              print("\nüîç DISCOVERING ALL AVAILABLE FILES...")
              
              all_files = []
              base_urls = [
                  "https://api.github.com/repos/delldevmann/proxy-scraper/contents/results",
              ]
              
              session = requests.Session()
              session.headers.update({
                  'User-Agent': 'ProxyMapBot/3.0 (+https://github.com/comprehensive-proxy-mapper)',
                  'Accept': 'application/vnd.github.v3+json',
                  'Accept-Encoding': 'gzip, deflate'
              })
              
              for api_url in base_urls:
                  try:
                      print(f"üì° Scanning: {api_url}")
                      response = session.get(api_url, timeout=30)
                      
                      if response.status_code != 200:
                          print(f"‚ùå API error {response.status_code}")
                          continue
                      
                      files = response.json()
                      if not isinstance(files, list):
                          print(f"‚ùå Unexpected response format")
                          continue
                      
                      source_files = []
                      for file in files:
                          if not isinstance(file, dict):
                              continue
                          
                          filename = file.get('name', '')
                          download_url = file.get('download_url', '')
                          file_size = file.get('size', 0)
                          
                          # Accept comprehensive file patterns
                          if (filename.endswith('.json') and 
                              any(pattern in filename.lower() for pattern in [
                                  'proxy', 'all_proxies', 'socks', 'http', 'https',
                                  'elite', 'anonymous', 'transparent'
                              ]) and 
                              file_size > 100):  # Skip tiny files
                              source_files.append((download_url, filename, file_size))
                      
                      print(f"‚úÖ Found {len(source_files)} proxy files from this source")
                      all_files.extend(source_files)
                      
                  except Exception as e:
                      print(f"‚ùå Error scanning {api_url}: {e}")
              
              # Sort by size (larger files first) and date
              def extract_datetime(filename):
                  date_match = re.search(r'(\d{8})', filename)
                  time_match = re.search(r'(\d{6})', filename)
                  date_str = date_match.group(1) if date_match else '00000000'
                  time_str = time_match.group(1) if time_match else '000000'
                  return date_str + time_str
              
              # Remove duplicates and sort
              unique_files = {}
              for url, filename, size in all_files:
                  key = filename
                  if key not in unique_files or size > unique_files[key][2]:
                      unique_files[key] = (url, filename, size)
              
              sorted_files = list(unique_files.values())
              # Sort by datetime (newest first), then by size (largest first)
              sorted_files.sort(key=lambda x: (extract_datetime(x[1]), x[2]), reverse=True)
              
              print(f"\nüìä DISCOVERY COMPLETE:")
              print(f"   üìÅ Total unique files: {len(sorted_files)}")
              print(f"   üíæ Total size: {sum(f[2] for f in sorted_files) / (1024*1024):.1f} MB")
              
              return sorted_files
          
          def enhanced_proxy_processor(file_data, filename):
              """Advanced proxy data processor with comprehensive format support"""
              proxies = []
              
              if not file_data or not isinstance(file_data, (dict, list)):
                  return proxies
              
              # Strategy 1: New nested schema with protocol separation
              if isinstance(file_data, dict) and "proxies" in file_data:
                  proxies_by_type = file_data.get("proxies", {})
                  
                  for protocol_type, protocol_proxies in proxies_by_type.items():
                      if not isinstance(protocol_proxies, dict):
                          continue
                      
                      for proxy_key, proxy_data in protocol_proxies.items():
                          try:
                              if not isinstance(proxy_data, dict):
                                  continue
                              
                              location = proxy_data.get('location', {})
                              lat = location.get('lat')
                              lon = location.get('lon')
                              
                              if lat is not None and lon is not None:
                                  lat, lon = float(lat), float(lon)
                                  
                                  if -90 <= lat <= 90 and -180 <= lon <= 180:
                                      ip = proxy_data.get('ip', proxy_key.split(':')[0] if ':' in proxy_key else proxy_key)
                                      port = proxy_key.split(':')[1] if ':' in proxy_key else proxy_data.get('port', 'Unknown')
                                      
                                      proxy_obj = {
                                          "ip": ip,
                                          "port": str(port),
                                          "latency": proxy_data.get('latency_ms', proxy_data.get('latency', 9999)),
                                          "anonymity": proxy_data.get('anonymity', 'Unknown'),
                                          "lat": lat,
                                          "lon": lon,
                                          "city": location.get('city', 'Unknown'),
                                          "country": location.get('country', 'Unknown'),
                                          "code": location.get('countryCode', 'xx').lower(),
                                          "isp": location.get('isp', 'N/A'),
                                          "source": filename,
                                          "protocol": proxy_data.get('type', protocol_type),
                                          "last_checked": proxy_data.get('last_checked', 'Unknown'),
                                          "success_rate": proxy_data.get('success_rate', None),
                                          "response_time": proxy_data.get('response_time', None),
                                          "ssl_support": proxy_data.get('ssl_support', False),
                                          "google_passed": proxy_data.get('google_passed', False)
                                      }
                                      proxies.append(proxy_obj)
                          except (ValueError, TypeError, KeyError):
                              continue
                  
                  return proxies
              
              # Strategy 2: Direct IP:PORT mapping
              if isinstance(file_data, dict):
                  ip_port_entries = [k for k in file_data.keys() if ':' in str(k) and '.' in str(k)]
                  
                  if len(ip_port_entries) > 5:
                      for ip_port, proxy_info in file_data.items():
                          if ':' not in str(ip_port) or not isinstance(proxy_info, dict):
                              continue
                          
                          try:
                              ip, port = str(ip_port).split(':', 1)
                              
                              if 'location' in proxy_info:
                                  location = proxy_info['location']
                                  lat = location.get('lat')
                                  lon = location.get('lon')
                                  
                                  if lat is not None and lon is not None:
                                      lat, lon = float(lat), float(lon)
                                      
                                      if -90 <= lat <= 90 and -180 <= lon <= 180:
                                          proxy_obj = {
                                              "ip": ip,
                                              "port": port,
                                              "latency": proxy_info.get('latency_ms', proxy_info.get('latency', 9999)),
                                              "anonymity": proxy_info.get('anonymity', 'Unknown'),
                                              "lat": lat,
                                              "lon": lon,
                                              "city": location.get('city', 'Unknown'),
                                              "country": location.get('country', 'Unknown'),
                                              "code": location.get('countryCode', 'xx').lower(),
                                              "isp": location.get('isp', 'N/A'),
                                              "source": filename,
                                              "protocol": proxy_info.get('type', proxy_info.get('protocol', 'http')),
                                              "last_checked": proxy_info.get('last_checked', 'Unknown'),
                                              "success_rate": proxy_info.get('success_rate', None),
                                              "response_time": proxy_info.get('response_time', None),
                                              "ssl_support": proxy_info.get('ssl_support', False),
                                              "google_passed": proxy_info.get('google_passed', False)
                                          }
                                          proxies.append(proxy_obj)
                          except (ValueError, TypeError, KeyError):
                              continue
                      
                      return proxies
              
              # Strategy 3: List format processing
              proxy_candidates = []
              
              if isinstance(file_data, list):
                  proxy_candidates = file_data
              elif isinstance(file_data, dict):
                  # Check for common list keys
                  for key in ['proxies', 'data', 'results', 'items', 'proxy_list', 'servers']:
                      if key in file_data and isinstance(file_data[key], list):
                          proxy_candidates = file_data[key]
                          break
                  
                  # Check protocol-specific keys
                  if not proxy_candidates:
                      for key in ['http', 'https', 'socks4', 'socks5', 'elite', 'anonymous']:
                          if key in file_data:
                              if isinstance(file_data[key], list):
                                  proxy_candidates.extend(file_data[key])
                              elif isinstance(file_data[key], dict):
                                  if 'proxies' in file_data[key] and isinstance(file_data[key]['proxies'], list):
                                      proxy_candidates.extend(file_data[key]['proxies'])
              
              # Process candidates
              for proxy in proxy_candidates:
                  if not isinstance(proxy, dict):
                      continue
                  
                  try:
                      # Extract IP address
                      ip = None
                      for field in ['ip', 'host', 'address', 'addr', 'proxy', 'server', 'endpoint']:
                          if field in proxy and proxy[field]:
                              ip = str(proxy[field])
                              if ':' in ip:
                                  ip = ip.split(':')[0]
                              break
                      
                      if not ip or '.' not in ip:
                          continue
                      
                      # Extract geolocation
                      lat, lon = None, None
                      location = {}
                      
                      # Multiple location field strategies
                      location_sources = ['location', 'geo', 'geolocation', 'coords']
                      for loc_key in location_sources:
                          if loc_key in proxy and isinstance(proxy[loc_key], dict):
                              location = proxy[loc_key]
                              break
                      
                      if location:
                          lat = location.get('lat', location.get('latitude'))
                          lon = location.get('lon', location.get('longitude', location.get('lng')))
                      else:
                          # Direct coordinate fields
                          for lat_field in ['lat', 'latitude', 'y']:
                              if lat_field in proxy:
                                  lat = proxy[lat_field]
                                  break
                          for lon_field in ['lon', 'longitude', 'lng', 'x']:
                              if lon_field in proxy:
                                  lon = proxy[lon_field]
                                  break
                          location = proxy
                      
                      if lat is None or lon is None:
                          continue
                      
                      lat, lon = float(lat), float(lon)
                      
                      if -90 <= lat <= 90 and -180 <= lon <= 180:
                          # Extract port
                          port = 'Unknown'
                          for port_field in ['port', 'Port', 'p']:
                              if port_field in proxy:
                                  port = str(proxy[port_field])
                                  break
                          
                          if port == 'Unknown' and ':' in str(proxy.get('ip', '')):
                              port = str(proxy['ip']).split(':')[1]
                          
                          # Extract country code
                          country_code = 'xx'
                          for code_field in ['countryCode', 'country_code', 'cc', 'code']:
                              if code_field in location:
                                  country_code = str(location[code_field]).lower()
                                  break
                          
                          proxy_obj = {
                              "ip": ip,
                              "port": port,
                              "latency": proxy.get('latency_ms', proxy.get('latency', proxy.get('delay', proxy.get('response_time', 9999)))),
                              "anonymity": proxy.get('anonymity', proxy.get('anon', proxy.get('level', 'Unknown'))),
                              "lat": lat,
                              "lon": lon,
                              "city": location.get('city', proxy.get('city', 'Unknown')),
                              "country": location.get('country', proxy.get('country', 'Unknown')),
                              "code": country_code,
                              "isp": location.get('isp', proxy.get('isp', proxy.get('org', 'N/A'))),
                              "source": filename,
                              "protocol": proxy.get('protocol', proxy.get('type', proxy.get('scheme', 'http'))),
                              "last_checked": proxy.get('last_checked', proxy.get('checked', proxy.get('updated', 'Unknown'))),
                              "success_rate": proxy.get('success_rate', proxy.get('uptime', None)),
                              "response_time": proxy.get('response_time', proxy.get('ping', None)),
                              "ssl_support": proxy.get('ssl_support', proxy.get('https', False)),
                              "google_passed": proxy.get('google_passed', proxy.get('google', False))
                          }
                          proxies.append(proxy_obj)
                  except (ValueError, TypeError, KeyError, IndexError):
                      continue
              
              return proxies
          
          def process_files_in_batches(all_files, batch_size=CHUNK_SIZE):
              """Process files in batches to manage memory and avoid timeouts"""
              print(f"\nüîÑ PROCESSING {len(all_files)} FILES IN BATCHES OF {batch_size}")
              
              all_proxies = []
              unique_ips = set()
              processed_files = []
              failed_files = []
              total_batches = (len(all_files) + batch_size - 1) // batch_size
              
              session = requests.Session()
              session.headers.update({
                  'User-Agent': 'ProxyMapBot/3.0 Comprehensive',
                  'Accept': 'application/json',
                  'Connection': 'keep-alive'
              })
              
              for batch_num in range(total_batches):
                  start_idx = batch_num * batch_size
                  end_idx = min(start_idx + batch_size, len(all_files))
                  batch_files = all_files[start_idx:end_idx]
                  
                  print(f"\n--- BATCH {batch_num + 1}/{total_batches} ---")
                  print(f"üìÇ Processing files {start_idx + 1}-{end_idx} of {len(all_files)}")
                  
                  def process_single_file(file_info):
                      url, filename, size = file_info
                      try:
                          file_data = fetch_with_retry(session, url)
                          if file_data:
                              file_proxies = enhanced_proxy_processor(file_data, filename)
                              return file_proxies, filename, True, size
                          return [], filename, False, size
                      except Exception as e:
                          print(f"‚ùå {filename}: {str(e)[:50]}")
                          return [], filename, False, size
                  
                  # Process batch with threading
                  batch_proxies = []
                  with ThreadPoolExecutor(max_workers=MAX_CONCURRENT_REQUESTS) as executor:
                      future_to_file = {executor.submit(process_single_file, file_info): file_info 
                                       for file_info in batch_files}
                      
                      for future in as_completed(future_to_file):
                          try:
                              file_proxies, filename, success, size = future.result()
                              
                              if success:
                                  processed_files.append(filename)
                                  
                                  # Add only truly unique proxies
                                  new_proxies = []
                                  for proxy in file_proxies:
                                      proxy_key = f"{proxy['ip']}:{proxy['port']}"
                                      if proxy_key not in unique_ips:
                                          unique_ips.add(proxy_key)
                                          new_proxies.append(proxy)
                                  
                                  if new_proxies:
                                      batch_proxies.extend(new_proxies)
                                      print(f"‚úÖ {filename}: +{len(new_proxies)} unique ({size/1024:.1f}KB)")
                                  else:
                                      print(f"‚ÑπÔ∏è  {filename}: No new proxies ({size/1024:.1f}KB)")
                              else:
                                  failed_files.append(filename)
                                  print(f"‚ùå {filename}: Failed to process")
                              
                          except Exception as e:
                              print(f"‚ùå Batch processing error: {e}")
                  
                  # Add batch results
                  all_proxies.extend(batch_proxies)
                  
                  print(f"üìä Batch {batch_num + 1} Results:")
                  print(f"   ‚úÖ New proxies: {len(batch_proxies):,}")
                  print(f"   üìà Total collected: {len(all_proxies):,}")
                  print(f"   üíæ Memory usage: ~{len(all_proxies) * 0.5 / 1024:.1f} MB")
                  
                  # Memory management
                  if len(all_proxies) > MAX_TOTAL_PROXIES:
                      print(f"üéØ Reached maximum target of {MAX_TOTAL_PROXIES:,} proxies")
                      break
                  
                  # Garbage collection between batches
                  gc.collect()
                  
                  # Small delay between batches
                  if batch_num < total_batches - 1:
                      time.sleep(2)
              
              session.close()
              return all_proxies, processed_files, failed_files
          
          # Execute comprehensive collection
          start_time = time.time()
          
          # Discovery phase
          all_files = discover_all_files()
          if not all_files:
              print("‚ùå No files discovered! Creating sample data...")
              # Enhanced sample data
              sample_proxies = [
                  {"ip": "1.1.1.1", "port": "8080", "lat": 40.7128, "lon": -74.0060, "country": "United States", "city": "New York", "code": "us", "latency": 45, "protocol": "http", "anonymity": "high", "isp": "Cloudflare", "source": "sample"},
                  {"ip": "8.8.8.8", "port": "3128", "lat": 37.7749, "lon": -122.4194, "country": "United States", "city": "San Francisco", "code": "us", "latency": 32, "protocol": "https", "anonymity": "medium", "isp": "Google", "source": "sample"},
                  {"ip": "208.67.222.222", "port": "8080", "lat": 51.5074, "lon": -0.1278, "country": "United Kingdom", "city": "London", "code": "gb", "latency": 67, "protocol": "http", "anonymity": "high", "isp": "OpenDNS", "source": "sample"}
              ]
              all_proxies = sample_proxies
              processed_files = ["sample_data"]
              failed_files = []
          else:
              # Processing phase
              all_proxies, processed_files, failed_files = process_files_in_batches(all_files)
          
          collection_time = time.time() - start_time
          
          print(f"\n{'='*60}")
          print(f"üéâ COMPREHENSIVE COLLECTION COMPLETE!")
          print(f"{'='*60}")
          print(f"‚è±Ô∏è  Total time: {collection_time/60:.1f} minutes")
          print(f"üìä Total unique proxies: {len(all_proxies):,}")
          print(f"‚úÖ Files processed: {len(processed_files)}")
          print(f"‚ùå Files failed: {len(failed_files)}")
          print(f"üìà Success rate: {len(processed_files)/(len(processed_files) + len(failed_files))*100:.1f}%")
          
          if not all_proxies:
              print("‚ùå No proxy data collected")
              exit(1)
          
          # Enhanced data analysis
          df = pd.DataFrame(all_proxies)
          df['latency'] = pd.to_numeric(df['latency'], errors='coerce').fillna(9999)
          
          print(f"\nüìà DETAILED ANALYSIS:")
          print(f"üåç Countries: {df['country'].nunique()}")
          print(f"üèôÔ∏è Cities: {df['city'].nunique()}")
          print(f"üè¢ ISPs: {df['isp'].nunique()}")
          print(f"‚ö° Avg latency: {df[df['latency'] < 9999]['latency'].mean():.0f}ms")
          print(f"üîí Protocols: {', '.join(df['protocol'].value_counts().head(5).index.tolist())}")
          
          # Performance breakdown
          fast_count = len(df[df['latency'] < 1000])
          medium_count = len(df[(df['latency'] >= 1000) & (df['latency'] < 2000)])
          slow_count = len(df[df['latency'] >= 2000])
          
          print(f"\nüöÄ Performance Distribution:")
          print(f"   Fast (<1s): {fast_count:,} ({fast_count/len(all_proxies)*100:.1f}%)")
          print(f"   Medium (1-2s): {medium_count:,} ({medium_count/len(all_proxies)*100:.1f}%)")
          print(f"   Slow (>2s): {slow_count:,} ({slow_count/len(all_proxies)*100:.1f}%)")
          
          # Create output directory
          os.makedirs("public", exist_ok=True)
          
          # Generate enhanced interactive map
          print(f"\nüó∫Ô∏è GENERATING INTERACTIVE MAP...")
          
          lat_center = df["lat"].median()
          lon_center = df["lon"].median()
          
          # Create professional map
          m = folium.Map(
              location=[lat_center, lon_center],
              zoom_start=2,
              tiles=None,  # We'll add custom tiles
              control_scale=True,
              prefer_canvas=True,
              zoom_control=True,
              scrollWheelZoom=True,
              doubleClickZoom=True,
              keyboard=True,
              attributionControl=False
          )
          
          # Add multiple tile layers
          folium.TileLayer(
              tiles='https://{s}.basemaps.cartocdn.com/dark_all/{z}/{x}/{y}{r}.png',
              attr='CartoDB',
              name='Dark Theme',
              control=True
          ).add_to(m)
          
          folium.TileLayer(
              tiles='OpenStreetMap',
              name='Street Map',
              control=True
          ).add_to(m)
          
          folium.TileLayer(
              tiles='https://server.arcgisonline.com/ArcGIS/rest/services/World_Imagery/MapServer/tile/{z}/{y}/{x}',
              attr='Esri',
              name='Satellite',
              control=True
          ).add_to(m)
          
          # Add enhanced plugins
          Fullscreen(position='topright', force_separate_button=True).add_to(m)
          
          MiniMap(
              toggle_display=True,
              tile_layer='https://{s}.basemaps.cartocdn.com/dark_all/{z}/{x}/{y}{r}.png',
              position='bottomright',
              width=150, height=150,
              minimized=True
          ).add_to(m)
          
          MousePosition(
              position='bottomleft',
              separator=' | ',
              empty_string='',
              lng_first=False,
              num_digits=3,
              prefix='Coordinates: '
          ).add_to(m)
          
          # Add layer control
          folium.LayerControl(position='topright').add_to(m)
          
          # Smart sampling for performance
          display_limit = 15000  # Increased for comprehensive display
          if len(df) > display_limit:
              print(f"üìå Sampling {display_limit:,} proxies for optimal map performance...")
              # Stratified sampling - more fast proxies, fewer slow ones
              fast_sample = min(len(df[df['latency'] < 1000]), int(display_limit * 0.5))
              medium_sample = min(len(df[(df['latency'] >= 1000) & (df['latency'] < 2000)]), int(display_limit * 0.3))
              slow_sample = min(len(df[df['latency'] >= 2000]), int(display_limit * 0.2))
              
              df_fast = df[df['latency'] < 1000].sample(n=fast_sample, random_state=42) if fast_sample > 0 else pd.DataFrame()
              df_medium = df[(df['latency'] >= 1000) & (df['latency'] < 2000)].sample(n=medium_sample, random_state=42) if medium_sample > 0 else pd.DataFrame()
              df_slow = df[df['latency'] >= 2000].sample(n=slow_sample, random_state=42) if slow_sample > 0 else pd.DataFrame()
              
              df_sample = pd.concat([df_fast, df_medium, df_slow], ignore_index=True)
              print(f"üìä Selected {len(df_sample):,} representative proxies")
          else:
              df_sample = df
          
          # Create marker clusters by protocol
          protocol_clusters = {}
          for protocol in df_sample['protocol'].unique():
              cluster = MarkerCluster(
                  name=f"{protocol.upper()} Proxies",
                  overlay=True,
                  control=True,
                  options={
                      'maxClusterRadius': 40,
                      'disableClusteringAtZoom': 12,
                      'spiderfyOnMaxZoom': True,
                      'showCoverageOnHover': False,
                      'zoomToBoundsOnClick': True,
                      'singleMarkerMode': False
                  }
              )
              protocol_clusters[protocol] = cluster
              cluster.add_to(m)
          
          # Enhanced marker generation
          print(f"üìç
