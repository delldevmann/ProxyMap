name: Generate Enhanced Proxy Map - All Sources

on:
  schedule:
    - cron: '0 */6 * * *'  # Run every 6 hours
  workflow_dispatch:  # Allow manual triggering

permissions:
  contents: write
  pages: write
  id-token: write

jobs:
  build-and-deploy:
    runs-on: ubuntu-22.04
    
    environment:
      name: github-pages
      url: ${{ steps.deployment.outputs.page_url }}
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          pip install --upgrade pip
          pip install folium requests matplotlib pandas seaborn

      - name: Generate proxy map from ALL sources
        run: |
          python3 << 'EOF'
          import requests, folium, pandas as pd, matplotlib.pyplot as plt
          from folium.plugins import Fullscreen, MiniMap, FloatImage, MarkerCluster
          import os, re, json
          from datetime import datetime
          import time
          from concurrent.futures import ThreadPoolExecutor, as_completed
          import random

          def fetch_with_retry(url, max_retries=3, timeout=60):
              """Fetch URL with retry logic"""
              for attempt in range(max_retries):
                  try:
                      print(f"  Attempt {attempt + 1}/{max_retries} for {url}")
                      response = requests.get(url, timeout=timeout, headers={
                          'User-Agent': 'ProxyMapBot/1.0',
                          'Accept': 'application/json'
                      })
                      
                      if response.status_code == 200:
                          return response.json()
                      elif response.status_code == 429:
                          wait_time = 2 ** attempt
                          print(f"  Rate limited, waiting {wait_time}s...")
                          time.sleep(wait_time)
                      else:
                          print(f"  HTTP {response.status_code}")
                          
                  except Exception as e:
                      print(f"  Error on attempt {attempt + 1}: {e}")
                  
                  if attempt < max_retries - 1:
                      time.sleep(2 ** attempt)
              
              return None

          def find_all_proxy_files():
              """Find ALL proxy files from the GitHub repo"""
              try:
                  api_url = "https://api.github.com/repos/delldevmann/proxy-scraper/contents/results"
                  response = requests.get(api_url, timeout=30)
                  
                  if response.status_code != 200:
                      print(f"GitHub API returned status {response.status_code}")
                      return []
                      
                  files = response.json()
                  print(f"Found {len(files)} files in results directory")
                  
                  proxy_files = []
                  for file in files:
                      filename = file['name']
                      # Include ALL JSON files that might contain proxies
                      if filename.endswith('.json'):
                          print(f"Found proxy file: {filename}")
                          proxy_files.append((file['download_url'], filename))
                  
                  # Sort files to prioritize newer dates
                  def extract_date(filename):
                      # Try to extract date from filename (e.g., all_proxies_20250301_013951.json)
                      match = re.search(r'(\d{8})', filename)
                      return match.group(1) if match else '00000000'
                  
                  proxy_files.sort(key=lambda x: extract_date(x[1]), reverse=True)
                  
                  print(f"Total proxy files found: {len(proxy_files)}")
                  return proxy_files
                      
              except Exception as e:
                  print(f"Error finding proxy files: {e}")
                  return []

          def process_proxy_data(file_data, filename):
              """Universal proxy data processor that handles ANY format"""
              proxies = []
              
              if not file_data:
                  return proxies
              
              print(f"Processing {filename}: {type(file_data)}")
              
              # Method 1: New schema with "proxies" key containing protocols
              if isinstance(file_data, dict) and "proxies" in file_data:
                  print(f"  Found new schema format with 'proxies' key")
                  proxies_by_type = file_data.get("proxies", {})
                  
                  for protocol_type, protocol_proxies in proxies_by_type.items():
                      if not isinstance(protocol_proxies, dict):
                          continue
                          
                      print(f"  Processing {protocol_type} proxies: {len(protocol_proxies)} entries")
                      
                      for proxy_key, proxy_data in protocol_proxies.items():
                          try:
                              location = proxy_data.get('location', {})
                              lat = location.get('lat')
                              lon = location.get('lon')
                              
                              if lat is not None and lon is not None:
                                  lat, lon = float(lat), float(lon)
                                  
                                  if -90 <= lat <= 90 and -180 <= lon <= 180:
                                      port = proxy_key.split(':')[1] if ':' in proxy_key else 'Unknown'
                                      
                                      proxies.append({
                                          "ip": proxy_data.get('ip', proxy_key.split(':')[0] if ':' in proxy_key else proxy_key),
                                          "port": port,
                                          "latency": proxy_data.get('latency_ms', 9999),
                                          "anonymity": proxy_data.get('anonymity', 'Unknown'),
                                          "lat": lat,
                                          "lon": lon,
                                          "city": location.get('city', 'Unknown'),
                                          "country": location.get('country', 'Unknown'),
                                          "code": location.get('countryCode', 'xx').lower(),
                                          "isp": location.get('isp', 'N/A'),
                                          "source": filename,
                                          "protocol": proxy_data.get('type', protocol_type),
                                          "last_checked": proxy_data.get('last_checked', 'Unknown')
                                      })
                          except Exception as e:
                              continue
                  
                  return proxies
              
              # Method 2: Direct IP:PORT mapping at root level
              if isinstance(file_data, dict):
                  # Check if we have IP:PORT pattern keys
                  ip_port_entries = [k for k in file_data.keys() if ':' in k and '.' in k]
                  if len(ip_port_entries) > 10:  # Likely a proxy file
                      print(f"  Found IP:PORT pattern in keys ({len(ip_port_entries)} entries)")
                      
                      for ip_port, proxy_info in file_data.items():
                          if ':' not in ip_port or not isinstance(proxy_info, dict):
                              continue
                          
                          try:
                              ip, port = ip_port.split(':', 1)
                              
                              if 'location' in proxy_info:
                                  location = proxy_info['location']
                                  lat = location.get('lat')
                                  lon = location.get('lon')
                                  
                                  if lat is not None and lon is not None:
                                      lat, lon = float(lat), float(lon)
                                      
                                      if -90 <= lat <= 90 and -180 <= lon <= 180:
                                          proxies.append({
                                              "ip": ip,
                                              "port": port,
                                              "latency": proxy_info.get('latency_ms', proxy_info.get('latency', 9999)),
                                              "anonymity": proxy_info.get('anonymity', 'Unknown'),
                                              "lat": lat,
                                              "lon": lon,
                                              "city": location.get('city', 'Unknown'),
                                              "country": location.get('country', 'Unknown'),
                                              "code": location.get('countryCode', 'xx').lower(),
                                              "isp": location.get('isp', 'N/A'),
                                              "source": filename,
                                              "protocol": proxy_info.get('type', proxy_info.get('protocol', 'http')),
                                              "last_checked": proxy_info.get('last_checked', 'Unknown')
                                          })
                          except Exception:
                              continue
                      
                      return proxies
              
              # Method 3: List of proxy objects
              proxy_candidates = []
              
              if isinstance(file_data, list):
                  proxy_candidates = file_data
              elif isinstance(file_data, dict):
                  # Look for common patterns
                  for key in ['proxies', 'data', 'results', 'items']:
                      if key in file_data and isinstance(file_data[key], list):
                          proxy_candidates = file_data[key]
                          break
                  
                  # Check protocol-specific keys
                  if not proxy_candidates:
                      for key in ['http', 'https', 'socks4', 'socks5']:
                          if key in file_data:
                              if isinstance(file_data[key], list):
                                  proxy_candidates.extend(file_data[key])
                              elif isinstance(file_data[key], dict):
                                  # Could be nested structure
                                  sub_data = file_data[key]
                                  if 'proxies' in sub_data and isinstance(sub_data['proxies'], list):
                                      proxy_candidates.extend(sub_data['proxies'])
              
              print(f"  Found {len(proxy_candidates)} proxy candidates")
              
              # Process candidates
              for proxy in proxy_candidates:
                  if not isinstance(proxy, dict):
                      continue
                  
                  try:
                      # Extract IP
                      ip = None
                      for field in ['ip', 'host', 'address', 'addr', 'proxy', 'server']:
                          if field in proxy:
                              ip = proxy[field]
                              if ip and ':' in ip:
                                  ip = ip.split(':')[0]
                              break
                      
                      if not ip:
                          continue
                      
                      # Extract location
                      lat, lon = None, None
                      location = {}
                      
                      if 'location' in proxy and isinstance(proxy['location'], dict):
                          location = proxy['location']
                          lat = location.get('lat', location.get('latitude'))
                          lon = location.get('lon', location.get('longitude'))
                      elif 'geo' in proxy and isinstance(proxy['geo'], dict):
                          location = proxy['geo']
                          lat = location.get('lat', location.get('latitude'))
                          lon = location.get('lon', location.get('longitude'))
                      elif 'lat' in proxy and 'lon' in proxy:
                          lat = proxy['lat']
                          lon = proxy['lon']
                          location = proxy
                      elif 'latitude' in proxy and 'longitude' in proxy:
                          lat = proxy['latitude']
                          lon = proxy['longitude']
                          location = proxy
                      
                      if lat is None or lon is None:
                          continue
                      
                      lat, lon = float(lat), float(lon)
                      
                      if -90 <= lat <= 90 and -180 <= lon <= 180:
                          # Extract port
                          port = proxy.get('port', proxy.get('Port', 'Unknown'))
                          if port == 'Unknown' and ':' in str(proxy.get('ip', '')):
                              port = proxy['ip'].split(':')[1]
                          
                          # Get country info
                          country = location.get('country', proxy.get('country', 'Unknown'))
                          country_code = location.get('countryCode', location.get('country_code', proxy.get('country_code', 'xx')))
                          
                          proxies.append({
                              "ip": ip,
                              "port": str(port),
                              "latency": proxy.get('latency_ms', proxy.get('latency', proxy.get('delay', 9999))),
                              "anonymity": proxy.get('anonymity', proxy.get('anon', 'Unknown')),
                              "lat": lat,
                              "lon": lon,
                              "city": location.get('city', proxy.get('city', 'Unknown')),
                              "country": country,
                              "code": country_code.lower() if country_code else 'xx',
                              "isp": location.get('isp', proxy.get('isp', 'N/A')),
                              "source": filename,
                              "protocol": proxy.get('protocol', proxy.get('type', 'http')),
                              "last_checked": proxy.get('last_checked', proxy.get('checked', 'Unknown'))
                          })
                  except Exception:
                      continue
              
              print(f"✅ Extracted {len(proxies)} valid proxies from {filename}")
              return proxies

          # Initialize data collection
          all_proxies = []
          unique_ips = set()
          data_sources = []
          
          print("=== COMPREHENSIVE PROXY DATA COLLECTION ===")
          print(f"Starting at: {datetime.now()}")
          
          # Get ALL proxy files
          print("\nFinding ALL proxy files...")
          proxy_files = find_all_proxy_files()
          
          if not proxy_files:
              print("❌ No proxy files found!")
              exit(1)
          
          print(f"\n📊 Will process {len(proxy_files)} proxy files")
          
          # Process files in batches to avoid overwhelming the system
          BATCH_SIZE = 5
          MAX_FILES = 50  # Increase limit to get more data
          
          files_to_process = proxy_files[:MAX_FILES]
          processed_count = 0
          
          for i in range(0, len(files_to_process), BATCH_SIZE):
              batch = files_to_process[i:i + BATCH_SIZE]
              print(f"\n--- Processing batch {i//BATCH_SIZE + 1}/{(len(files_to_process) + BATCH_SIZE - 1)//BATCH_SIZE} ---")
              
              def fetch_and_process_file(file_info):
                  url, filename = file_info
                  print(f"\nFetching {filename}...")
                  
                  file_data = fetch_with_retry(url, max_retries=2, timeout=90)
                  if file_data:
                      return process_proxy_data(file_data, filename), filename
                  return [], filename
              
              # Process batch with threading
              with ThreadPoolExecutor(max_workers=3) as executor:
                  future_to_file = {executor.submit(fetch_and_process_file, file_info): file_info 
                                   for file_info in batch}
                  
                  for future in as_completed(future_to_file):
                      try:
                          file_proxies, filename = future.result()
                          if file_proxies:
                              # Add only unique IPs
                              new_proxies = []
                              for proxy in file_proxies:
                                  if proxy["ip"] not in unique_ips:
                                      unique_ips.add(proxy["ip"])
                                      new_proxies.append(proxy)
                              
                              if new_proxies:
                                  all_proxies.extend(new_proxies)
                                  data_sources.append(f"{filename}: {len(new_proxies)} unique")
                                  print(f"✅ Added {len(new_proxies)} new unique proxies from {filename}")
                              else:
                                  print(f"ℹ️  No new unique proxies from {filename}")
                          
                          processed_count += 1
                          
                      except Exception as e:
                          print(f"❌ Error processing file: {e}")
              
              # Small delay between batches
              if i + BATCH_SIZE < len(files_to_process):
                  time.sleep(1)

          print(f"\n=== FINAL RESULTS ===")
          print(f"🎉 Total unique proxies collected: {len(all_proxies):,}")
          print(f"📊 Successfully processed: {processed_count}/{len(files_to_process)} files")
          print(f"📁 Data sources with unique proxies: {len(data_sources)}")
          
          if data_sources:
              print("\n📋 Sources breakdown:")
              for i, source in enumerate(data_sources[:20], 1):  # Show top 20
                  print(f"  {i}. {source}")
              if len(data_sources) > 20:
                  print(f"  ... and {len(data_sources) - 20} more sources")

          if not all_proxies:
              print("❌ No proxy data collected - cannot generate map")
              exit(1)

          # Analyze the data
          df = pd.DataFrame(all_proxies)
          
          # Clean up latency data
          df['latency'] = pd.to_numeric(df['latency'], errors='coerce').fillna(9999)
          
          print(f"\n📈 ANALYSIS:")
          print(f"  🌍 Countries represented: {df['country'].nunique()}")
          print(f"  ⚡ Average latency: {df[df['latency'] < 9999]['latency'].mean():.0f}ms")
          print(f"  🚀 Fast proxies (<1000ms): {len(df[df['latency'] < 1000]):,}")
          print(f"  🟡 Medium proxies (1000-2000ms): {len(df[(df['latency'] >= 1000) & (df['latency'] < 2000)]):,}")
          print(f"  🔴 Slow proxies (>2000ms): {len(df[df['latency'] >= 2000]):,}")
          
          # Protocol distribution
          protocol_dist = df['protocol'].value_counts()
          print(f"\n  📡 Protocol distribution:")
          for protocol, count in protocol_dist.items():
              print(f"    {protocol}: {count:,}")
          
          # Top countries
          top_countries = df['country'].value_counts().head(15)
          print(f"\n  🏆 Top 15 countries:")
          for country, count in top_countries.items():
              print(f"    {country}: {count:,}")

          # Generate the enhanced map
          print(f"\n🗺️ Generating interactive map...")
          
          # Calculate map center
          lat_center = df["lat"].median()
          lon_center = df["lon"].median()

          # Create map with better styling
          m = folium.Map(
              location=[lat_center, lon_center], 
              zoom_start=2, 
              tiles="CartoDB dark_matter",
              control_scale=True,
              prefer_canvas=True  # Better performance for many markers
          )
          
          # Add map plugins
          Fullscreen().add_to(m)
          MiniMap(toggle_display=True, position='bottomright').add_to(m)
          
          # Use marker clustering for performance
          marker_cluster = MarkerCluster(
              name="Proxy Locations",
              overlay=True,
              control=True,
              options={
                  'maxClusterRadius': 50,
                  'disableClusteringAtZoom': 10,
                  'spiderfyOnMaxZoom': True,
                  'showCoverageOnHover': False
              }
          ).add_to(m)

          # Sample markers if too many (for performance)
          if len(df) > 10000:
              print(f"  📌 Too many markers ({len(df):,}), sampling 10,000 for performance...")
              df_sample = df.sample(n=10000, random_state=42)
          else:
              df_sample = df

          # Add markers
          for _, row in df_sample.iterrows():
              # Color coding based on latency
              if row["latency"] < 1000:
                  color = '#00ff00'  # Green
                  size = 8
              elif row["latency"] < 2000:
                  color = '#ffa500'  # Orange
                  size = 6
              else:
                  color = '#ff0000'  # Red
                  size = 4
              
              # Country flag
              flag_url = f"https://flagcdn.com/24x18/{row['code']}.png"
              
              # Simplified popup for performance
              popup_content = f"""
              <div style='font-family: Arial, sans-serif; font-size: 12px; width: 280px;'>
                <div style='background: #667eea; color: white; padding: 8px; margin: -5px -5px 10px -5px;'>
                  <img src='{flag_url}' style='vertical-align: middle; margin-right: 8px;' onerror="this.style.display='none'"> 
                  <strong>{row['country']}</strong>
                </div>
                <table style='width: 100%;'>
                  <tr><td>🌐 IP:Port</td><td>{row['ip']}:{row['port']}</td></tr>
                  <tr><td>📍 Location</td><td>{row['city']}</td></tr>
                  <tr><td>⚡ Latency</td><td>{row['latency']} ms</td></tr>
                  <tr><td>📡 Protocol</td><td>{row['protocol'].upper()}</td></tr>
                  <tr><td>📊 Source</td><td style='font-size: 10px;'>{row['source'][:30]}...</td></tr>
                </table>
              </div>
              """
              
              folium.CircleMarker(
                  location=[row["lat"], row["lon"]],
                  radius=size,
                  color='white',
                  weight=1,
                  fill=True,
                  fillColor=color,
                  fillOpacity=0.8,
                  popup=folium.Popup(popup_content, max_width=300),
                  tooltip=f"{row['country']} | {row['latency']}ms"
              ).add_to(marker_cluster)

          # Create visualizations
          os.makedirs("public", exist_ok=True)
          
          # Enhanced charts
          plt.style.use('dark_background')
          fig = plt.figure(figsize=(16, 10))
          
          # Layout: 2x2 grid
          ax1 = plt.subplot(2, 2, 1)  # Top countries
          ax2 = plt.subplot(2, 2, 2)  # Protocol distribution
          ax3 = plt.subplot(2, 2, 3)  # Latency distribution
          ax4 = plt.subplot(2, 2, 4)  # Data sources
          
          # 1. Top countries bar chart
          top_countries_15 = df["country"].value_counts().nlargest(15)
          colors = plt.cm.viridis(range(len(top_countries_15)))
          
          bars = ax1.barh(range(len(top_countries_15)), top_countries_15.values, color=colors)
          ax1.set_yticks(range(len(top_countries_15)))
          ax1.set_yticklabels(top_countries_15.index, fontsize=10)
          ax1.set_xlabel("Number of Proxies", fontweight='bold')
          ax1.set_title(f"Top 15 Countries ({len(all_proxies):,} total proxies)", fontweight='bold', pad=20)
          ax1.grid(axis='x', alpha=0.3)
          
          # Add value labels
          for i, (country, count) in enumerate(top_countries_15.items()):
              ax1.text(count + max(top_countries_15.values) * 0.01, i, f'{count:,}', 
                      va='center', ha='left', fontsize=9)
          
          # 2. Protocol distribution pie chart
          protocol_counts = df['protocol'].value_counts()
          ax2.pie(protocol_counts.values, labels=[f"{p}\n({c:,})" for p, c in protocol_counts.items()], 
                  autopct='%1.1f%%', colors=plt.cm.Set3(range(len(protocol_counts))),
                  startangle=90)
          ax2.set_title("Protocol Distribution", fontweight='bold', pad=20)
          
          # 3. Latency distribution histogram
          latency_data = df[df['latency'] < 9999]['latency']
          ax3.hist(latency_data, bins=50, color='skyblue', edgecolor='white', alpha=0.7)
          ax3.set_xlabel("Latency (ms)", fontweight='bold')
          ax3.set_ylabel("Count", fontweight='bold')
          ax3.set_title(f"Latency Distribution (avg: {latency_data.mean():.0f}ms)", fontweight='bold')
          ax3.grid(axis='y', alpha=0.3)
          ax3.axvline(1000, color='green', linestyle='--', label='Fast (<1s)')
          ax3.axvline(2000, color='orange', linestyle='--', label='Medium (<2s)')
          ax3.legend()
          
          # 4. Data sources info
          ax4.axis('off')
          sources_text = f"📊 Data Collection Summary\n\n"
          sources_text += f"Total Files Processed: {processed_count}\n"
          sources_text += f"Unique Proxies Found: {len(all_proxies):,}\n"
          sources_text += f"Countries Covered: {df['country'].nunique()}\n\n"
          sources_text += f"Top Data Sources:\n"
          for i, source in enumerate(data_sources[:10], 1):
              sources_text += f"{i}. {source}\n"
          if len(data_sources) > 10:
              sources_text += f"\n... and {len(data_sources) - 10} more sources"
          
          ax4.text(0.1, 0.9, sources_text, transform=ax4.transAxes, 
                  fontsize=11, verticalalignment='top',
                  bbox=dict(boxstyle='round', facecolor='#2e2e2e', alpha=0.8))
          
          plt.tight_layout()
          plt.savefig("public/analysis_charts.png", dpi=150, bbox_inches='tight', 
                     facecolor='#1a1a1a', edgecolor='none')
          plt.close()

          # Enhanced statistics panel
          fast_count = len(df[df['latency'] < 1000])
          medium_count = len(df[(df['latency'] >= 1000) & (df['latency'] < 2000)])
          slow_count = len(df[df['latency'] >= 2000])
          
          stats_html = f"""
          <div style="position: fixed; 
                      top: 10px; left: 10px; width: 340px; 
                      background: linear-gradient(135deg, rgba(0,0,0,0.95) 0%, rgba(30,30,30,0.95) 100%); 
                      border: 2px solid #667eea; border-radius: 12px;
                      z-index: 9999; font-size: 13px; padding: 16px;
                      box-shadow: 0 8px 32px rgba(0,0,0,0.3);
                      color: white; font-family: 'Segoe UI', Arial, sans-serif;">
          
          <div style="text-align: center; margin-bottom: 12px;">
            <h3 style="margin: 0; color: #667eea; font-size: 18px;">🗺️ Global Proxy Network Map</h3>
            <div style="font-size: 11px; color: #aaa; margin-top: 4px;">
              Comprehensive Proxy Intelligence from {len(data_sources)} Sources
            </div>
          </div>
          
          <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 8px; margin-bottom: 12px;">
            <div style="background: rgba(102, 126, 234, 0.2); padding: 8px; border-radius: 6px; text-align: center;">
              <div style="font-size: 20px; font-weight: bold; color: #667eea;">{len(all_proxies):,}</div>
              <div style="font-size: 10px; color: #ccc;">Total Proxies</div>
            </div>
            <div style="background: rgba(118, 75, 162, 0.2); padding: 8px; border-radius: 6px; text-align: center;">
              <div style="font-size: 20px; font-weight: bold; color: #764ba2;">{df['country'].nunique()}</div>
              <div style="font-size: 10px; color: #ccc;">Countries</div>
            </div>
          </div>
          
          <div style="background: rgba(255,255,255,0.05); padding: 10px; border-radius: 6px; margin-bottom: 12px;">
            <div style="font-size: 11px; color: #ccc; margin-bottom: 6px; text-align: center;">Performance Distribution</div>
            <div style="display: flex; justify-content: space-between; font-size: 11px;">
              <div style="text-align: center; flex: 1;">
                <div style="color: #00ff00; font-weight: bold; font-size: 16px;">{fast_count:,}</div>
                <div style="color: #aaa; font-size: 9px;">Fast<br/>&lt;1s</div>
              </div>
              <div style="text-align: center; flex: 1;">
                <div style="color: #ffa500; font-weight: bold; font-size: 16px;">{medium_count:,}</div>
                <div style="color: #aaa; font-size: 9px;">Medium<br/>1-2s</div>
              </div>
              <div style="text-align: center; flex: 1;">
                <div style="color: #ff0000; font-weight: bold; font-size: 16px;">{slow_count:,}</div>
                <div style="color: #aaa; font-size: 9px;">Slow<br/>&gt;2s</div>
              </div>
            </div>
          </div>
          
          <div style="margin-bottom: 10px;">
            <div style="display: flex; justify-content: space-between; align-items: center; margin-bottom: 4px;">
              <span style="font-size: 12px;">⚡ Avg Lat
