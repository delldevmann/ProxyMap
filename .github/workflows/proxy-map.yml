name: Generate Enhanced Proxy Map - All Sources

on:
  schedule:
    - cron: '0 */6 * * *'  # Run every 6 hours
  workflow_dispatch:  # Allow manual triggering

permissions:
  contents: write
  pages: write
  id-token: write

jobs:
  build-and-deploy:
    runs-on: ubuntu-22.04
    
    environment:
      name: github-pages
      url: ${{ steps.deployment.outputs.page_url }}
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          pip install --upgrade pip
          pip install folium requests matplotlib pandas seaborn

      - name: Generate proxy map and analytics
        run: |
          python3 << 'EOF'
          import requests, folium, pandas as pd, matplotlib.pyplot as plt
          from folium.plugins import Fullscreen, MiniMap, MarkerCluster, MousePosition
          import os, re, json
          from datetime import datetime
          import time
          from concurrent.futures import ThreadPoolExecutor, as_completed

          def fetch_with_retry(url, max_retries=3, timeout=60):
              """Fetch URL with retry logic"""
              for attempt in range(max_retries):
                  try:
                      print(f"  Attempt {attempt + 1}/{max_retries} for {url}")
                      response = requests.get(url, timeout=timeout, headers={
                          'User-Agent': 'ProxyMapBot/1.0',
                          'Accept': 'application/json'
                      })
                      
                      if response.status_code == 200:
                          return response.json()
                      elif response.status_code == 429:
                          wait_time = 2 ** attempt
                          print(f"  Rate limited, waiting {wait_time}s...")
                          time.sleep(wait_time)
                      else:
                          print(f"  HTTP {response.status_code}")
                          
                  except Exception as e:
                      print(f"  Error on attempt {attempt + 1}: {e}")
                  
                  if attempt < max_retries - 1:
                      time.sleep(2 ** attempt)
              
              return None

          def find_all_proxy_files():
              """Find ALL proxy files from the GitHub repo"""
              try:
                  api_url = "https://api.github.com/repos/delldevmann/proxy-scraper/contents/results"
                  response = requests.get(api_url, timeout=30)
                  
                  if response.status_code != 200:
                      print(f"GitHub API returned status {response.status_code}")
                      return []
                      
                  files = response.json()
                  proxy_files = []
                  for file in files:
                      filename = file['name']
                      if filename.endswith('.json'):
                          proxy_files.append((file['download_url'], filename))
                  
                  # Sort by date (newest first)
                  def extract_date(filename):
                      match = re.search(r'(\d{8})', filename)
                      return match.group(1) if match else '00000000'
                  
                  proxy_files.sort(key=lambda x: extract_date(x[1]), reverse=True)
                  return proxy_files[:25]  # Limit to 25 files for faster processing
                      
              except Exception as e:
                  print(f"Error finding proxy files: {e}")
                  return []

          def process_proxy_data(file_data, filename):
              """Universal proxy data processor"""
              proxies = []
              
              if not file_data:
                  return proxies
              
              # Handle new schema with "proxies" key
              if isinstance(file_data, dict) and "proxies" in file_data:
                  proxies_by_type = file_data.get("proxies", {})
                  
                  for protocol_type, protocol_proxies in proxies_by_type.items():
                      if not isinstance(protocol_proxies, dict):
                          continue
                      
                      for proxy_key, proxy_data in protocol_proxies.items():
                          try:
                              location = proxy_data.get('location', {})
                              lat = location.get('lat')
                              lon = location.get('lon')
                              
                              if lat is not None and lon is not None:
                                  lat, lon = float(lat), float(lon)
                                  
                                  if -90 <= lat <= 90 and -180 <= lon <= 180:
                                      port = proxy_key.split(':')[1] if ':' in proxy_key else 'Unknown'
                                      
                                      proxies.append({
                                          "ip": proxy_data.get('ip', proxy_key.split(':')[0] if ':' in proxy_key else proxy_key),
                                          "port": port,
                                          "latency": proxy_data.get('latency_ms', 9999),
                                          "anonymity": proxy_data.get('anonymity', 'Unknown'),
                                          "lat": lat,
                                          "lon": lon,
                                          "city": location.get('city', 'Unknown'),
                                          "country": location.get('country', 'Unknown'),
                                          "code": location.get('countryCode', 'xx').lower(),
                                          "isp": location.get('isp', 'N/A'),
                                          "source": filename,
                                          "protocol": proxy_data.get('type', protocol_type),
                                          "last_checked": proxy_data.get('last_checked', 'Unknown')
                                      })
                          except Exception:
                              continue
                  
                  return proxies
              
              # Handle direct IP:PORT mapping
              if isinstance(file_data, dict):
                  ip_port_entries = [k for k in file_data.keys() if ':' in k and '.' in k]
                  if len(ip_port_entries) > 10:
                      for ip_port, proxy_info in file_data.items():
                          if ':' not in ip_port or not isinstance(proxy_info, dict):
                              continue
                          
                          try:
                              ip, port = ip_port.split(':', 1)
                              
                              if 'location' in proxy_info:
                                  location = proxy_info['location']
                                  lat = location.get('lat')
                                  lon = location.get('lon')
                                  
                                  if lat is not None and lon is not None:
                                      lat, lon = float(lat), float(lon)
                                      
                                      if -90 <= lat <= 90 and -180 <= lon <= 180:
                                          proxies.append({
                                              "ip": ip,
                                              "port": port,
                                              "latency": proxy_info.get('latency_ms', 9999),
                                              "anonymity": proxy_info.get('anonymity', 'Unknown'),
                                              "lat": lat,
                                              "lon": lon,
                                              "city": location.get('city', 'Unknown'),
                                              "country": location.get('country', 'Unknown'),
                                              "code": location.get('countryCode', 'xx').lower(),
                                              "isp": location.get('isp', 'N/A'),
                                              "source": filename,
                                              "protocol": proxy_info.get('type', 'http'),
                                              "last_checked": proxy_info.get('last_checked', 'Unknown')
                                          })
                          except Exception:
                              continue
              
              return proxies

          # Main execution
          print("=== PROXY DATA COLLECTION STARTED ===")
          
          all_proxies = []
          unique_ips = set()
          
          proxy_files = find_all_proxy_files()
          print(f"Found {len(proxy_files)} proxy files to process")
          
          if not proxy_files:
              print("No proxy files found, creating sample data...")
              # Create sample data for demonstration
              sample_proxies = [
                  {"ip": "1.1.1.1", "port": "8080", "lat": 40.7128, "lon": -74.0060, "country": "United States", "city": "New York", "code": "us", "latency": 45, "protocol": "http", "anonymity": "high", "isp": "Cloudflare", "source": "sample"},
                  {"ip": "8.8.8.8", "port": "3128", "lat": 37.7749, "lon": -122.4194, "country": "United States", "city": "San Francisco", "code": "us", "latency": 32, "protocol": "https", "anonymity": "medium", "isp": "Google", "source": "sample"},
                  {"ip": "1.1.1.2", "port": "8080", "lat": 51.5074, "lon": -0.1278, "country": "United Kingdom", "city": "London", "code": "gb", "latency": 67, "protocol": "http", "anonymity": "high", "isp": "Cloudflare", "source": "sample"}
              ]
              all_proxies = sample_proxies
          else:
              # Process files
              for url, filename in proxy_files[:10]:  # Process up to 10 files
                  print(f"\nProcessing {filename}...")
                  
                  file_data = fetch_with_retry(url, max_retries=2, timeout=30)
                  if file_data:
                      file_proxies = process_proxy_data(file_data, filename)
                      
                      # Add only unique IPs
                      new_proxies = []
                      for proxy in file_proxies:
                          if proxy["ip"] not in unique_ips:
                              unique_ips.add(proxy["ip"])
                              new_proxies.append(proxy)
                      
                      if new_proxies:
                          all_proxies.extend(new_proxies)
                          print(f"Added {len(new_proxies)} unique proxies")
                  
                  # Limit total proxies for performance
                  if len(all_proxies) > 5000:
                      break

          print(f"\nTotal unique proxies collected: {len(all_proxies)}")

          if not all_proxies:
              print("No data collected, exiting...")
              exit(1)

          # Create DataFrame for analysis
          df = pd.DataFrame(all_proxies)
          df['latency'] = pd.to_numeric(df['latency'], errors='coerce').fillna(9999)

          # Create the directory
          os.makedirs("public", exist_ok=True)

          # Generate interactive map
          print("Generating interactive map...")
          
          lat_center = df["lat"].median()
          lon_center = df["lon"].median()

          # Create map
          m = folium.Map(
              location=[lat_center, lon_center], 
              zoom_start=2, 
              tiles="CartoDB dark_matter",
              control_scale=True,
              prefer_canvas=True
          )
          
          # Add plugins
          Fullscreen(position='topright').add_to(m)
          
          MiniMap(
              toggle_display=True,
              tile_layer="CartoDB dark_matter",
              position='bottomright',
              width=150,
              height=150,
              minimized=True
          ).add_to(m)
          
          # Add marker clustering
          marker_cluster = MarkerCluster(
              name="Proxy Locations",
              options={'maxClusterRadius': 60}
          ).add_to(m)

          # Sample markers if too many
          if len(df) > 2000:
              df_sample = df.sample(n=2000, random_state=42)
          else:
              df_sample = df

          # Add markers
          for _, row in df_sample.iterrows():
              # Color by latency
              if row["latency"] < 1000:
                  color = '#00ff00'
                  size = 8
              elif row["latency"] < 2000:
                  color = '#ffa500'
                  size = 6
              else:
                  color = '#ff0000'
                  size = 4
              
              popup_content = f"""
              <div style='font-family: Arial; font-size: 12px; width: 250px;'>
                <b>{row['country']} - {row['city']}</b><br>
                <b>IP:</b> {row['ip']}:{row['port']}<br>
                <b>Latency:</b> {row['latency']}ms<br>
                <b>Protocol:</b> {row['protocol']}<br>
                <b>ISP:</b> {row['isp']}<br>
              </div>
              """
              
              folium.CircleMarker(
                  location=[row["lat"], row["lon"]],
                  radius=size,
                  color='white',
                  weight=1,
                  fill=True,
                  fillColor=color,
                  fillOpacity=0.8,
                  popup=folium.Popup(popup_content, max_width=300),
                  tooltip=f"{row['country']} | {row['latency']}ms"
              ).add_to(marker_cluster)

          # Add statistics panel
          fast_count = len(df[df['latency'] < 1000])
          medium_count = len(df[(df['latency'] >= 1000) & (df['latency'] < 2000)])
          slow_count = len(df[df['latency'] >= 2000])
          
          stats_html = f'''
          <div style="position: fixed; top: 20px; left: 20px; width: 250px; 
                      background: rgba(0, 0, 0, 0.8); color: white; 
                      border-radius: 8px; padding: 20px; z-index: 1000; 
                      font-family: Arial; font-size: 12px;">
          
          <h3 style="margin: 0 0 15px 0;">Global Proxy Network</h3>
          
          <div style="margin-bottom: 15px;">
            <div style="font-size: 24px; font-weight: bold;">{len(all_proxies):,}</div>
            <div style="color: #ccc;">Total Proxies</div>
          </div>
          
          <div style="margin-bottom: 10px;">
            <div style="display: flex; justify-content: space-between;">
              <span>ðŸŸ¢ Fast (&lt;1s)</span>
              <span>{fast_count:,}</span>
            </div>
          </div>
          
          <div style="margin-bottom: 10px;">
            <div style="display: flex; justify-content: space-between;">
              <span>ðŸŸ¡ Medium (1-2s)</span>
              <span>{medium_count:,}</span>
            </div>
          </div>
          
          <div style="margin-bottom: 15px;">
            <div style="display: flex; justify-content: space-between;">
              <span>ðŸ”´ Slow (&gt;2s)</span>
              <span>{slow_count:,}</span>
            </div>
          </div>
          
          <div style="border-top: 1px solid #555; padding-top: 10px;">
            <div style="display: flex; justify-content: space-between;">
              <span>Countries:</span>
              <span>{df['country'].nunique()}</span>
            </div>
            <div style="display: flex; justify-content: space-between;">
              <span>Avg Latency:</span>
              <span>{df[df['latency'] < 9999]['latency'].mean():.0f}ms</span>
            </div>
          </div>
          </div>
          '''
          
          m.get_root().html.add_child(folium.Element(stats_html))

          # Save map
          m.save("public/index.html")

          # Generate analytics charts
          print("Generating analytics charts...")
          
          plt.style.use('dark_background')
          fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))
          fig.patch.set_facecolor('#0a0a0a')

          # 1. Top countries
          top_countries = df["country"].value_counts().head(10)
          ax1.barh(range(len(top_countries)), top_countries.values, color='#00ff88')
          ax1.set_yticks(range(len(top_countries)))
          ax1.set_yticklabels(top_countries.index)
          ax1.set_title("Top 10 Countries", color='white')
          ax1.grid(axis='x', alpha=0.3)

          # 2. Protocol distribution
          protocol_counts = df['protocol'].value_counts()
          colors = ['#00ff88', '#00ccff', '#ff6b6b', '#ffd93d']
          ax2.pie(protocol_counts.values, labels=protocol_counts.index, autopct='%1.1f%%', 
                  colors=colors[:len(protocol_counts)])
          ax2.set_title("Protocol Distribution", color='white')

          # 3. Latency distribution
          latency_data = df[df['latency'] < 5000]['latency']
          ax3.hist(latency_data, bins=30, color='#00ff88', alpha=0.7)
          ax3.set_xlabel("Latency (ms)")
          ax3.set_ylabel("Count")
          ax3.set_title(f"Latency Distribution (Avg: {latency_data.mean():.0f}ms)", color='white')
          ax3.grid(alpha=0.3)

          # 4. Performance summary
          performance_data = [fast_count, medium_count, slow_count]
          performance_labels = ['Fast (<1s)', 'Medium (1-2s)', 'Slow (>2s)']
          performance_colors = ['#00ff00', '#ffa500', '#ff0000']
          
          ax4.bar(performance_labels, performance_data, color=performance_colors)
          ax4.set_title("Performance Distribution", color='white')
          ax4.set_ylabel("Count")
          for i, v in enumerate(performance_data):
              ax4.text(i, v + max(performance_data) * 0.01, str(v), 
                      ha='center', va='bottom', color='white')

          plt.tight_layout()
          plt.savefig("public/analytics.png", dpi=120, bbox_inches='tight', 
                     facecolor='#0a0a0a')
          plt.close()

          # Save summary data
          summary_data = {
              "timestamp": datetime.now().isoformat(),
              "total_proxies": len(all_proxies),
              "countries": int(df['country'].nunique()),
              "average_latency": float(df[df['latency'] < 9999]['latency'].mean()),
              "protocol_distribution": protocol_counts.to_dict(),
              "top_countries": top_countries.head(5).to_dict(),
              "performance": {
                  "fast": int(fast_count),
                  "medium": int(medium_count),
                  "slow": int(slow_count)
              }
          }
          
          with open("public/summary.json", "w") as f:
              json.dump(summary_data, f, indent=2)

          print(f"\nâœ… SUCCESS!")
          print(f"ðŸ“Š Generated map with {len(all_proxies):,} proxies")
          print(f"ðŸŒ Covering {df['country'].nunique()} countries")
          print(f"ðŸ“ˆ Average latency: {df[df['latency'] < 9999]['latency'].mean():.0f}ms")
          
          EOF

      - name: Create .nojekyll file
        run: echo "" > public/.nojekyll

      - name: Upload Pages artifact
        uses: actions/upload-pages-artifact@v3
        with:
          path: './public'

      - name: Deploy to GitHub Pages
        id: deployment
        uses: actions/deploy-pages@v4
