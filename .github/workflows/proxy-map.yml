name: Generate Enhanced Proxy Map - ALL PROXIES

on:
  schedule:
    - cron: '0 */6 * * *'  # Run every 6 hours
  workflow_dispatch:  # Allow manual triggering

permissions:
  contents: write
  pages: write
  id-token: write

jobs:
  build-and-deploy:
    runs-on: ubuntu-22.04
    
    environment:
      name: github-pages
      url: ${{ steps.deployment.outputs.page_url }}
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          pip install --upgrade pip
          pip install folium requests matplotlib pandas seaborn numpy

      - name: Generate comprehensive proxy map
        run: |
          python3 << 'EOF'
          import requests, folium, pandas as pd, matplotlib.pyplot as plt, numpy as np
          from folium.plugins import Fullscreen, MiniMap, MarkerCluster, MousePosition
          import os, re, json
          from datetime import datetime
          import time
          from concurrent.futures import ThreadPoolExecutor, as_completed

          # Configuration for maximum data collection
          MAX_CONCURRENT_REQUESTS = 5
          MAX_TOTAL_PROXIES = 50000  # Increased limit
          MAX_FILES_TO_PROCESS = 100  # Process more files
          REQUEST_TIMEOUT = 45
          MAX_RETRIES = 3

          def fetch_with_retry(url, max_retries=MAX_RETRIES, timeout=REQUEST_TIMEOUT):
              """Enhanced fetch with better error handling"""
              for attempt in range(max_retries):
                  try:
                      response = requests.get(url, timeout=timeout, headers={
                          'User-Agent': 'ProxyMapBot/2.0 (+https://github.com/your-username/ProxyMap)',
                          'Accept': 'application/json, text/plain, */*',
                          'Accept-Encoding': 'gzip, deflate',
                          'Connection': 'keep-alive'
                      })
                      
                      if response.status_code == 200:
                          try:
                              return response.json()
                          except json.JSONDecodeError:
                              print(f"  Invalid JSON in {url}")
                              return None
                      elif response.status_code == 429:
                          wait_time = min(2 ** attempt, 30)  # Cap wait time
                          print(f"  Rate limited, waiting {wait_time}s...")
                          time.sleep(wait_time)
                      elif response.status_code == 404:
                          print(f"  File not found: {url}")
                          return None
                      else:
                          print(f"  HTTP {response.status_code}")
                          
                  except requests.exceptions.Timeout:
                      print(f"  Timeout on attempt {attempt + 1}")
                  except requests.exceptions.RequestException as e:
                      print(f"  Request error: {e}")
                  
                  if attempt < max_retries - 1:
                      time.sleep(min(2 ** attempt, 10))
              
              return None

          def find_all_proxy_files():
              """Find ALL available proxy files from multiple sources"""
              all_files = []
              
              # Primary source
              sources = [
                  "https://api.github.com/repos/delldevmann/proxy-scraper/contents/results",
              ]
              
              for api_url in sources:
                  try:
                      print(f"Scanning: {api_url}")
                      response = requests.get(api_url, timeout=30, headers={
                          'User-Agent': 'ProxyMapBot/2.0'
                      })
                      
                      if response.status_code != 200:
                          print(f"  API returned status {response.status_code}")
                          continue
                          
                      files = response.json()
                      if not isinstance(files, list):
                          print(f"  Unexpected API response format")
                          continue
                      
                      source_files = []
                      for file in files:
                          if not isinstance(file, dict):
                              continue
                              
                          filename = file.get('name', '')
                          download_url = file.get('download_url', '')
                          
                          # Accept more file types
                          if (filename.endswith('.json') or 
                              'proxy' in filename.lower() or 
                              'all_proxies' in filename.lower()):
                              source_files.append((download_url, filename))
                      
                      print(f"  Found {len(source_files)} files from this source")
                      all_files.extend(source_files)
                      
                  except Exception as e:
                      print(f"  Error accessing {api_url}: {e}")
                      continue
              
              # Sort files by date (newest first) and remove duplicates
              def extract_date_time(filename):
                  # Try to extract date and time from filename
                  date_match = re.search(r'(\d{8})', filename)
                  time_match = re.search(r'(\d{6})', filename)
                  date_str = date_match.group(1) if date_match else '00000000'
                  time_str = time_match.group(1) if time_match else '000000'
                  return date_str + time_str
              
              # Remove duplicates by filename
              unique_files = {}
              for url, filename in all_files:
                  if filename not in unique_files or extract_date_time(filename) > extract_date_time(unique_files[filename][1]):
                      unique_files[filename] = (url, filename)
              
              sorted_files = list(unique_files.values())
              sorted_files.sort(key=lambda x: extract_date_time(x[1]), reverse=True)
              
              print(f"Total unique files found: {len(sorted_files)}")
              return sorted_files[:MAX_FILES_TO_PROCESS]

          def process_proxy_data(file_data, filename):
              """Enhanced universal proxy data processor"""
              proxies = []
              
              if not file_data:
                  return proxies
              
              print(f"  Processing {filename}: {type(file_data).__name__}")
              
              # Method 1: New schema with "proxies" key
              if isinstance(file_data, dict) and "proxies" in file_data:
                  proxies_by_type = file_data.get("proxies", {})
                  
                  for protocol_type, protocol_proxies in proxies_by_type.items():
                      if not isinstance(protocol_proxies, dict):
                          continue
                      
                      print(f"    Processing {protocol_type}: {len(protocol_proxies)} entries")
                      
                      for proxy_key, proxy_data in protocol_proxies.items():
                          try:
                              if not isinstance(proxy_data, dict):
                                  continue
                              
                              location = proxy_data.get('location', {})
                              lat = location.get('lat')
                              lon = location.get('lon')
                              
                              if lat is not None and lon is not None:
                                  lat, lon = float(lat), float(lon)
                                  
                                  if -90 <= lat <= 90 and -180 <= lon <= 180:
                                      port = proxy_key.split(':')[1] if ':' in proxy_key else 'Unknown'
                                      
                                      proxies.append({
                                          "ip": proxy_data.get('ip', proxy_key.split(':')[0] if ':' in proxy_key else proxy_key),
                                          "port": port,
                                          "latency": proxy_data.get('latency_ms', proxy_data.get('latency', 9999)),
                                          "anonymity": proxy_data.get('anonymity', 'Unknown'),
                                          "lat": lat,
                                          "lon": lon,
                                          "city": location.get('city', 'Unknown'),
                                          "country": location.get('country', 'Unknown'),
                                          "code": location.get('countryCode', 'xx').lower(),
                                          "isp": location.get('isp', 'N/A'),
                                          "source": filename,
                                          "protocol": proxy_data.get('type', protocol_type),
                                          "last_checked": proxy_data.get('last_checked', 'Unknown'),
                                          "response_time": proxy_data.get('response_time', None),
                                          "success_rate": proxy_data.get('success_rate', None)
                                      })
                          except Exception:
                              continue
                  
                  return proxies
              
              # Method 2: Direct IP:PORT mapping
              if isinstance(file_data, dict):
                  ip_port_entries = [k for k in file_data.keys() if ':' in k and '.' in k]
                  if len(ip_port_entries) > 5:  # Lower threshold
                      print(f"    Found IP:PORT format with {len(ip_port_entries)} entries")
                      
                      for ip_port, proxy_info in file_data.items():
                          if ':' not in ip_port or not isinstance(proxy_info, dict):
                              continue
                          
                          try:
                              ip, port = ip_port.split(':', 1)
                              
                              if 'location' in proxy_info:
                                  location = proxy_info['location']
                                  lat = location.get('lat')
                                  lon = location.get('lon')
                                  
                                  if lat is not None and lon is not None:
                                      lat, lon = float(lat), float(lon)
                                      
                                      if -90 <= lat <= 90 and -180 <= lon <= 180:
                                          proxies.append({
                                              "ip": ip,
                                              "port": port,
                                              "latency": proxy_info.get('latency_ms', proxy_info.get('latency', 9999)),
                                              "anonymity": proxy_info.get('anonymity', 'Unknown'),
                                              "lat": lat,
                                              "lon": lon,
                                              "city": location.get('city', 'Unknown'),
                                              "country": location.get('country', 'Unknown'),
                                              "code": location.get('countryCode', 'xx').lower(),
                                              "isp": location.get('isp', 'N/A'),
                                              "source": filename,
                                              "protocol": proxy_info.get('type', proxy_info.get('protocol', 'http')),
                                              "last_checked": proxy_info.get('last_checked', 'Unknown'),
                                              "response_time": proxy_info.get('response_time', None),
                                              "success_rate": proxy_info.get('success_rate', None)
                                          })
                          except Exception:
                              continue
                      
                      return proxies
              
              # Method 3: List format
              proxy_candidates = []
              
              if isinstance(file_data, list):
                  proxy_candidates = file_data
              elif isinstance(file_data, dict):
                  # Look for common list keys
                  for key in ['proxies', 'data', 'results', 'items', 'proxy_list']:
                      if key in file_data and isinstance(file_data[key], list):
                          proxy_candidates = file_data[key]
                          break
                  
                  # Check protocol-specific keys
                  if not proxy_candidates:
                      for key in ['http', 'https', 'socks4', 'socks5']:
                          if key in file_data:
                              if isinstance(file_data[key], list):
                                  proxy_candidates.extend(file_data[key])
                              elif isinstance(file_data[key], dict) and 'proxies' in file_data[key]:
                                  if isinstance(file_data[key]['proxies'], list):
                                      proxy_candidates.extend(file_data[key]['proxies'])
              
              if proxy_candidates:
                  print(f"    Processing {len(proxy_candidates)} proxy candidates")
                  
                  for proxy in proxy_candidates:
                      if not isinstance(proxy, dict):
                          continue
                      
                      try:
                          # Extract IP
                          ip = None
                          for field in ['ip', 'host', 'address', 'addr', 'proxy', 'server']:
                              if field in proxy:
                                  ip = proxy[field]
                                  if ip and ':' in str(ip):
                                      ip = str(ip).split(':')[0]
                                  break
                          
                          if not ip:
                              continue
                          
                          # Extract location
                          lat, lon = None, None
                          location = {}
                          
                          if 'location' in proxy and isinstance(proxy['location'], dict):
                              location = proxy['location']
                              lat = location.get('lat', location.get('latitude'))
                              lon = location.get('lon', location.get('longitude'))
                          elif 'geo' in proxy and isinstance(proxy['geo'], dict):
                              location = proxy['geo']
                              lat = location.get('lat', location.get('latitude'))
                              lon = location.get('lon', location.get('longitude'))
                          elif 'lat' in proxy and 'lon' in proxy:
                              lat = proxy['lat']
                              lon = proxy['lon']
                              location = proxy
                          elif 'latitude' in proxy and 'longitude' in proxy:
                              lat = proxy['latitude']
                              lon = proxy['longitude']
                              location = proxy
                          
                          if lat is None or lon is None:
                              continue
                          
                          lat, lon = float(lat), float(lon)
                          
                          if -90 <= lat <= 90 and -180 <= lon <= 180:
                              # Extract port
                              port = proxy.get('port', proxy.get('Port', 'Unknown'))
                              if port == 'Unknown' and ':' in str(proxy.get('ip', '')):
                                  port = str(proxy['ip']).split(':')[1]
                              
                              proxies.append({
                                  "ip": ip,
                                  "port": str(port),
                                  "latency": proxy.get('latency_ms', proxy.get('latency', proxy.get('delay', proxy.get('response_time', 9999)))),
                                  "anonymity": proxy.get('anonymity', proxy.get('anon', 'Unknown')),
                                  "lat": lat,
                                  "lon": lon,
                                  "city": location.get('city', proxy.get('city', 'Unknown')),
                                  "country": location.get('country', proxy.get('country', 'Unknown')),
                                  "code": (location.get('countryCode', location.get('country_code', proxy.get('country_code', 'xx')))).lower(),
                                  "isp": location.get('isp', proxy.get('isp', 'N/A')),
                                  "source": filename,
                                  "protocol": proxy.get('protocol', proxy.get('type', 'http')),
                                  "last_checked": proxy.get('last_checked', proxy.get('checked', 'Unknown')),
                                  "response_time": proxy.get('response_time', None),
                                  "success_rate": proxy.get('success_rate', None)
                              })
                      except Exception:
                          continue
              
              print(f"    ✅ Extracted {len(proxies)} valid proxies")
              return proxies

          # Main execution with enhanced data collection
          print("=== COMPREHENSIVE PROXY DATA COLLECTION ===")
          print(f"Target: Collect up to {MAX_TOTAL_PROXIES:,} proxies from all available sources")
          print(f"Started at: {datetime.now()}")
          
          all_proxies = []
          unique_ips = set()
          processed_files = []
          failed_files = []
          
          proxy_files = find_all_proxy_files()
          
          if not proxy_files:
              print("❌ No proxy files found!")
              # Create more comprehensive sample data
              sample_data = [
                  {"ip": "1.1.1.1", "port": "8080", "lat": 40.7128, "lon": -74.0060, "country": "United States", "city": "New York", "code": "us", "latency": 45, "protocol": "http", "anonymity": "high", "isp": "Cloudflare", "source": "sample"},
                  {"ip": "8.8.8.8", "port": "3128", "lat": 37.7749, "lon": -122.4194, "country": "United States", "city": "San Francisco", "code": "us", "latency": 32, "protocol": "https", "anonymity": "medium", "isp": "Google", "source": "sample"},
                  {"ip": "1.1.1.2", "port": "8080", "lat": 51.5074, "lon": -0.1278, "country": "United Kingdom", "city": "London", "code": "gb", "latency": 67, "protocol": "http", "anonymity": "high", "isp": "Cloudflare", "source": "sample"},
                  {"ip": "9.9.9.9", "port": "3128", "lat": 52.5200, "lon": 13.4050, "country": "Germany", "city": "Berlin", "code": "de", "latency": 89, "protocol": "https", "anonymity": "medium", "isp": "Quad9", "source": "sample"},
                  {"ip": "208.67.222.222", "port": "8080", "lat": 35.6762, "lon": 139.6503, "country": "Japan", "city": "Tokyo", "code": "jp", "latency": 156, "protocol": "http", "anonymity": "low", "isp": "OpenDNS", "source": "sample"}
              ]
              all_proxies = sample_data
              for proxy in sample_data:
                  unique_ips.add(proxy["ip"])
          else:
              print(f"\n🔄 Processing {len(proxy_files)} proxy files with {MAX_CONCURRENT_REQUESTS} concurrent requests...")
              
              # Process files in batches
              BATCH_SIZE = MAX_CONCURRENT_REQUESTS
              
              for i in range(0, len(proxy_files), BATCH_SIZE):
                  batch = proxy_files[i:i + BATCH_SIZE]
                  print(f"\n--- Batch {i//BATCH_SIZE + 1}/{(len(proxy_files) + BATCH_SIZE - 1)//BATCH_SIZE} ---")
                  
                  def process_file(file_info):
                      url, filename = file_info
                      print(f"Fetching {filename}...")
                      
                      file_data = fetch_with_retry(url)
                      if file_data:
                          return process_proxy_data(file_data, filename), filename, True
                      return [], filename, False
                  
                  # Process batch concurrently
                  with ThreadPoolExecutor(max_workers=MAX_CONCURRENT_REQUESTS) as executor:
                      future_to_file = {executor.submit(process_file, file_info): file_info 
                                       for file_info in batch}
                      
                      for future in as_completed(future_to_file):
                          try:
                              file_proxies, filename, success = future.result()
                              
                              if success:
                                  processed_files.append(filename)
                                  
                                  # Add only unique IPs
                                  new_proxies = []
                                  for proxy in file_proxies:
                                      if proxy["ip"] not in unique_ips:
                                          unique_ips.add(proxy["ip"])
                                          new_proxies.append(proxy)
                                  
                                  if new_proxies:
                                      all_proxies.extend(new_proxies)
                                      print(f"✅ {filename}: Added {len(new_proxies)} unique proxies (Total: {len(all_proxies):,})")
                                  else:
                                      print(f"ℹ️  {filename}: No new unique proxies")
                              else:
                                  failed_files.append(filename)
                                  print(f"❌ {filename}: Failed to process")
                              
                          except Exception as e:
                              print(f"❌ Error processing file: {e}")
                  
                  # Check if we've reached our target
                  if len(all_proxies) >= MAX_TOTAL_PROXIES:
                      print(f"\n🎯 Reached target of {MAX_TOTAL_PROXIES:,} proxies, stopping collection")
                      break
                  
                  # Small delay between batches
                  if i + BATCH_SIZE < len(proxy_files):
                      time.sleep(2)

          print(f"\n=== COLLECTION SUMMARY ===")
          print(f"🎉 Total unique proxies collected: {len(all_proxies):,}")
          print(f"✅ Successfully processed files: {len(processed_files)}")
          print(f"❌ Failed files: {len(failed_files)}")
          print(f"📊 Unique IP addresses: {len(unique_ips):,}")

          if not all_proxies:
              print("❌ No proxy data collected - cannot generate map")
              exit(1)

          # Enhanced data analysis
          df = pd.DataFrame(all_proxies)
          df['latency'] = pd.to_numeric(df['latency'], errors='coerce').fillna(9999)

          print(f"\n📈 DATA ANALYSIS:")
          print(f"  🌍 Countries represented: {df['country'].nunique()}")
          print(f"  🏙️ Cities represented: {df['city'].nunique()}")
          print(f"  ⚡ Average latency: {df[df['latency'] < 9999]['latency'].mean():.0f}ms")
          print(f"  🚀 Fast proxies (<1000ms): {len(df[df['latency'] < 1000]):,}")
          print(f"  🟡 Medium proxies (1000-2000ms): {len(df[(df['latency'] >= 1000) & (df['latency'] < 2000)]):,}")
          print(f"  🔴 Slow proxies (>2000ms): {len(df[df['latency'] >= 2000]):,}")

          # Enhanced map generation with performance optimizations
          os.makedirs("public", exist_ok=True)
          
          print(f"\n🗺️ Generating interactive map...")
          
          lat_center = df["lat"].median()
          lon_center = df["lon"].median()

          # Create enhanced map
          m = folium.Map(
              location=[lat_center, lon_center], 
              zoom_start=2, 
              tiles="CartoDB dark_matter",
              control_scale=True,
              prefer_canvas=True,
              zoom_control=True,
              scrollWheelZoom=True,
              doubleClickZoom=True,
              keyboard=True
          )
          
          # Add comprehensive plugins
          Fullscreen(position='topright', force_separate_button=True).add_to(m)
          
          MiniMap(
              toggle_display=True,
              tile_layer="CartoDB dark_matter", 
              position='bottomright',
              width=150, height=150,
              minimized=True
          ).add_to(m)
          
          MousePosition(
              position='bottomleft',
              separator=' | ',
              empty_string='',
              lng_first=False,
              num_digits=3
          ).add_to(m)

          # Smart sampling for performance
          if len(df) > 10000:
              print(f"  📌 Large dataset ({len(df):,} proxies), using smart sampling...")
              # Sample more from fast proxies, fewer from slow ones
              fast_df = df[df['latency'] < 1000].sample(n=min(len(df[df['latency'] < 1000]), 6000), random_state=42)
              medium_df = df[(df['latency'] >= 1000) & (df['latency'] < 2000)].sample(n=min(len(df[(df['latency'] >= 1000) & (df['latency'] < 2000)]), 3000), random_state=42)
              slow_df = df[df['latency'] >= 2000].sample(n=min(len(df[df['latency'] >= 2000]), 1000), random_state=42)
              df_sample = pd.concat([fast_df, medium_df, slow_df])
              print(f"  📊 Selected {len(df_sample):,} representative proxies for display")
          else:
              df_sample = df

          # Enhanced marker clustering
          marker_cluster = MarkerCluster(
              name="Proxy Locations",
              overlay=True,
              control=False,
              options={
                  'maxClusterRadius': 50,
                  'disableClusteringAtZoom': 10,
                  'spiderfyOnMaxZoom': True,
                  'showCoverageOnHover': False,
                  'zoomToBoundsOnClick': True
              }
          ).add_to(m)

          # Add enhanced markers
          for _, row in df_sample.iterrows():
              # Enhanced color coding
              if row["latency"] < 500:
                  color, size = '#00ff00', 10  # Bright green for very fast
              elif row["latency"] < 1000:
                  color, size = '#7fff00', 8   # Lime green for fast
              elif row["latency"] < 1500:
                  color, size = '#ffff00', 7   # Yellow for medium-fast
              elif row["latency"] < 2000:
                  color, size = '#ffa500', 6   # Orange for medium-slow
              elif row["latency"] < 3000:
                  color, size = '#ff6347', 5   # Red-orange for slow
              else:
                  color, size = '#ff0000', 4   # Red for very slow
              
              # Enhanced popup with more information
              popup_content = f"""
              <div style='font-family: -apple-system, BlinkMacSystemFont, Arial, sans-serif; font-size: 13px; width: 280px; line-height: 1.4;'>
                <div style='background: linear-gradient(45deg, #1a1a1a, #2a2a2a); color: white; padding: 12px; margin: -8px -8px 10px -8px; border-radius: 6px 6px 0 0;'>
                  <div style='font-size: 16px; font-weight: 600; margin-bottom: 4px;'>{row['country']} • {row['city']}</div>
                  <div style='font-size: 11px; opacity: 0.8;'>{row['isp']}</div>
                </div>
                
                <div style='padding: 0 8px 8px 8px;'>
                  <table style='width: 100%; border-collapse: collapse; font-size: 12px;'>
                    <tr><td style='padding: 3px 0; color: #666; width: 35%;'>Address:</td><td style='font-family: monospace; font-weight: 600;'>{row['ip']}:{row['port']}</td></tr>
                    <tr><td style='padding: 3px 0; color: #666;'>Protocol:</td><td style='text-transform: uppercase; font-weight: 600;'>{row['protocol']}</td></tr>
                    <tr><td style='padding: 3px 0; color: #666;'>Latency:</td><td style='font-weight: 600; color: {"#00ff00" if row["latency"] < 1000 else "#ffa500" if row["latency"] < 2000 else "#ff0000"};'>{row['latency']}ms</td></tr>
                    <tr><td style='padding: 3px 0; color: #666;'>Anonymity:</td><td style='font-weight: 600;'>{row['anonymity']}</td></tr>
                    <tr><td style='padding: 3px 0; color: #666;'>Source:</td><td style='font-size: 11px; opacity: 0.8;'>{row['source'][:25]}...</td></tr>
                  </table>
                </div>
              </div>
              """
              
              folium.CircleMarker(
                  location=[row["lat"], row["lon"]],
                  radius=size,
                  color='white',
                  weight=1,
                  fill=True,
                  fillColor=color,
                  fillOpacity=0.9,
                  popup=folium.Popup(popup_content, max_width=320),
                  tooltip=f"{row['country']} | {row['city']} | {row['latency']}ms | {row['protocol'].upper()}"
              ).add_to(marker_cluster)

          # Enhanced statistics and UI elements
          fast_count = len(df[df['latency'] < 1000])
          medium_count = len(df[(df['latency'] >= 1000) & (df['latency'] < 2000)])
          slow_count = len(df[df['latency'] >= 2000])
          
          # Enhanced statistics panel with more details
          stats_html = f'''
          <div style="position: fixed; top: 20px; left: 20px; width: 320px; 
                      background: rgba(15, 15, 15, 0.92); 
                      backdrop-filter: blur(15px);
                      border: 1px solid rgba(255, 255, 255, 0.15);
                      border-radius: 12px; padding: 24px; z-index: 1000; 
                      font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Arial, sans-serif;
                      box-shadow: 0 8px 32px rgba(0,0,0,0.6);
                      color: white; font-size: 13px; line-height: 1.5;">
          
          <div style="margin-bottom: 20px; border-bottom: 1px solid rgba(255,255,255,0.1); padding-bottom: 15px;">
            <h3 style="margin: 0; font-size: 18px; font-weight: 600; color: #fff;">🌐 Global Proxy Network</h3>
            <div style="font-size: 11px; color: #888; margin-top: 3px;">
              Real-time data from {len(processed_files)} sources • Updated {datetime.now().strftime('%H:%M UTC')}
            </div>
          </div>
          
          <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 15px; margin-bottom: 20px;">
            <div style="background: rgba(0, 255, 136, 0.1); border: 1px solid rgba(0, 255, 136, 0.2); padding: 15px; border-radius: 8px; text-align: center;">
              <div style="font-size: 26px; font-weight: 700; color: #00ff88;">{len(all_proxies):,}</div>
              <div style="font-size: 10px; color: #888; margin-top: 2px; text-transform: uppercase; letter-spacing: 0.5px;">Total Proxies</div>
            </div>
            <div style="background: rgba(0, 204, 255, 0.1); border: 1px solid rgba(0, 204, 255, 0.2); padding: 15px; border-radius: 8px; text-align: center;">
              <div style="font-size: 26px; font-weight: 700; color: #00ccff;">{df['country'].nunique()}</div>
              <div style="font-size: 10px; color: #888; margin-top: 2px; text-transform: uppercase; letter-spacing: 0.5px;">Countries</div>
            </div>
          </div>
          
          <div style="margin-bottom: 18px;">
            <div style="font-size: 11px; color: #888; margin-bottom: 10px; text-transform: uppercase; letter-spacing: 0.5px;">PERFORMANCE BREAKDOWN</div>
            
            <div style="display: flex; justify-content: space-between; align-items: center; margin-bottom: 6px;">
              <div style="display: flex; align-items: center;">
                <div style="width: 10px; height: 10px; background: #00ff00; border-radius: 50%; margin-right: 10px;"></div>
                <span style="font-size: 12px;">Fast (&lt;1000ms)</span>
              </div>
              <div style="display: flex; align-items: center; gap: 8px;">
                <span style="font-size: 12px; font-weight: 600;">{fast_count:,}</span>
                <span style="font-size: 10px; color: #888;">({fast_count/len(all_proxies)*100:.1f}%)</span>
              </div>
            </div>
            
            <div style="display: flex; justify-content: space-between; align-items: center; margin-bottom: 6px;">
              <div style="display: flex; align-items: center;">
                <div style="width: 10px; height: 10px; background: #ffa500; border-radius: 50%; margin-right: 10px;"></div>
                <span style="font-size: 12px;">Medium (1-2s)</span>
              </div>
              <div style="display: flex; align-items: center; gap: 8px;">
                <span style="font-size: 12px; font-weight: 600;">{medium_count:,}</span>
                <span style="font-size: 10px; color: #888;">({medium_count/len(all_proxies)*100:.1f}%)</span>
              </div>
            </div>
            
            <div style="display: flex; justify-content: space-between; align-items: center; margin-bottom: 6px;">
              <div style="display: flex; align-items: center;">
                <div style="width: 10px; height: 10px; background: #ff0000; border-radius: 50%; margin-right: 10px;"></div>
                <span style="font-size: 12px;">Slow (&gt;2s)</span>
              </div>
              <div style="display: flex; align-items: center; gap: 8px;">
                <span style="font-size: 12px; font-weight: 600;">{slow_count:,}</span>
                <span style="font-size: 10px; color: #888;">({slow_count/len(all_proxies)*100:.1f}%)</span>
              </div>
            </div>
          </div>
          
          <div style="padding-top: 15px; border-top: 1px solid rgba(255, 255, 255, 0.1); font-size: 11px;">
            <div style="display: flex; justify-content: space-between; margin-bottom: 6px;">
              <span style="color: #888;">Average Latency</span>
              <span style="font-weight: 600;">{df[df['latency'] < 9999]['latency'].mean():.0f}ms</span>
            </div>
            <div style="display: flex; justify-content: space-between; margin-bottom: 6px;">
              <span style="color: #888;">Cities Covered</span>
              <span style="font-weight: 600;">{df['city'].nunique():,}</span>
            </div>
            <div style="display: flex; justify-content: space-between; margin-bottom: 6px;">
              <span style="color: #888;">Protocols</span>
              <span style="font-weight: 600;">{df['protocol'].nunique()}</span>
            </div>
            <div style="display: flex; justify-content: space-between;">
              <span style="color: #888;">Success Rate</span>
              <span style="font-weight: 600;">{len(processed_files)}/{len(processed_files) + len(failed_files)} sources</span>
            </div>
          </div>
          </div>
          '''
          
          # Enhanced legend
          legend_html = '''
          <div style="position: fixed; bottom: 20px; left: 20px;
                      background: rgba(15, 15, 15, 0.92); 
                      backdrop-filter: blur(15px);
                      border: 1px solid rgba(255, 255, 255, 0.15);
                      border-radius: 8px; padding: 16px; z-index: 1000; 
                      font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Arial, sans-serif;
                      box-shadow: 0 4px 20px rgba(0,0,0,0.4);
                      color: white; font-size: 12px;">
          
          <div style="margin-bottom: 8px; font-weight: 600; font-size: 11px; color: #888; text-transform: uppercase; letter-spacing: 0.5px;">Response Time</div>
          <div style="display: flex; align-items: center; gap: 20px; flex-wrap: wrap;">
            <div style="display: flex; align-items: center; gap: 6px;">
              <div style="width: 12px; height: 12px; background: #00ff00; border-radius: 50%; border: 1px solid #fff;"></div>
              <span>&lt;500ms</span>
            </div>
            <div style="display: flex; align-items: center; gap: 6px;">
              <div style="width: 10px; height: 10px; background: #7fff00; border-radius: 50%; border: 1px solid #fff;"></div>
              <span>&lt;1s</span>
            </div>
            <div style="display: flex; align-items: center; gap: 6px;">
              <div style="width: 8px; height: 8px; background: #ffff00; border-radius: 50%; border: 1px solid #fff;"></div>
              <span>&lt;1.5s</span>
            </div>
            <div style="display: flex; align-items: center; gap: 6px;">
              <div style="width: 6px; height: 6px; background: #ffa500; border-radius: 50%; border: 1px solid #fff;"></div>
              <span>&lt;2s</span>
            </div>
            <div style="display: flex; align-items: center; gap: 6px;">
              <div style="width: 5px; height: 5px; background: #ff6347; border-radius: 50%; border: 1px solid #fff;"></div>
              <span>&lt;3s</span>
            </div>
            <div style="display: flex; align-items: center; gap: 6px;">
              <div style="width: 4px; height: 4px; background: #ff0000; border-radius: 50%; border: 1px solid #fff;"></div>
              <span>&gt;3s</span>
            </div>
          </div>
          </div>
          '''
          
          # Add custom CSS for enhanced styling
          custom_css = '''
          <style>
          .leaflet-control-attribution { display: none !important; }
          .leaflet-control-zoom { 
              background: rgba(15, 15, 15, 0.92) !important; 
              backdrop-filter: blur(15px);
              border: 1px solid rgba(255, 255, 255, 0.15) !important;
              border-radius: 8px !important;
              box-shadow: 0 4px 20px rgba(0,0,0,0.4) !important;
          }
          .leaflet-control-zoom a { 
              background-color: transparent !important;
              color: rgba(255, 255, 255, 0.9) !important;
              border-bottom: 1px solid rgba(255, 255, 255, 0.1) !important;
              transition: all 0.2s ease !important;
          }
          .leaflet-control-zoom a:hover { 
              background-color: rgba(255, 255, 255, 0.1) !important;
              color: white !important;
          }
          .leaflet-control-zoom a:last-child { border-bottom: none !important; }
          .leaflet-control-fullscreen a {
              background: rgba(15, 15, 15, 0.92) !important;
              backdrop-filter: blur(15px);
              border: 1px solid rgba(255, 255, 255, 0.15) !important;
              border-radius: 8px !important;
              box-shadow: 0 4px 20px rgba(0,0,0,0.4) !important;
              color: rgba(255, 255, 255, 0.9) !important;
          }
          .leaflet-control-mouseposition {
              background: rgba(15, 15, 15, 0.92) !important;
              backdrop-filter: blur(15px);
              color: rgba(255, 255, 255, 0.7) !important;
              padding: 6px 10px !important;
              font-size: 11px !important;
              border: 1px solid rgba(255, 255, 255, 0.15) !important;
              border-radius: 6px !important;
              box-shadow: 0 2px 10px rgba(0,0,0,0.3) !important;
          }
          .leaflet-control-minimap {
              border: 1px solid rgba(255, 255, 255, 0.15) !important;
              border-radius: 8px !important;
              box-shadow: 0 4px 20px rgba(0,0,0,0.4) !important;
              overflow: hidden;
          }
          </style>
          '''
          
          # Add all elements to map
          m.get_root().html.add_child(folium.Element(custom_css))
          m.get_root().html.add_child(folium.Element(stats_html))
          m.get_root().html.add_child(folium.Element(legend_html))

          # Save the enhanced map
          m.save("public/index.html")
          print(f"✅ Interactive map saved with {len(df_sample):,} markers displayed")

          # Generate comprehensive analytics charts
          print("📊 Generating comprehensive analytics...")
          
          plt.style.use('dark_background')
          fig = plt.figure(figsize=(20, 12))
          fig.patch.set_facecolor('#0a0a0a')
          
          # Create a complex grid layout
          gs = fig.add_gridspec(3, 4, hspace=0.3, wspace=0.3)
          
          # 1. Top 15 countries (larger)
          ax1 = fig.add_subplot(gs[0:2, 0:2])
          top_countries = df["country"].value_counts().head(15)
          bars = ax1.barh(range(len(top_countries)), top_countries.values, 
                         color=plt.cm.viridis(np.linspace(0, 1, len(top_countries))))
          ax1.set_yticks(range(len(top_countries)))
          ax1.set_yticklabels(top_countries.index, fontsize=11)
          ax1.set_xlabel("Number of Proxies", fontsize=12, color='white')
          ax1.set_title(f"Top 15 Countries ({top_countries.sum():,} proxies)", fontsize=14, color='white', pad=20)
          ax1.grid(axis='x', alpha=0.3, linestyle='--')
          ax1.spines['top'].set_visible(False)
          ax1.spines['right'].set_visible(False)
          
          # Add value labels
          for i, (country, count) in enumerate(top_countries.items()):
              ax1.text(count + max(top_countries.values) * 0.01, i, f'{count:,}', 
                      va='center', ha='left', fontsize=10, color='white', alpha=0.8)

          # 2. Protocol distribution (pie chart)
          ax2 = fig.add_subplot(gs[0, 2])
          protocol_counts = df['protocol'].value_counts()
          colors = ['#00ff88', '#00ccff', '#ff6b6b', '#ffd93d', '#a8e6cf']
          wedges, texts, autotexts = ax2.pie(protocol_counts.values, 
                                             labels=protocol_counts.index,
                                             autopct='%1.1f%%',
                                             colors=colors[:len(protocol_counts)],
                                             startangle=90,
                                             textprops={'fontsize': 10})
          ax2.set_title("Protocol Distribution", fontsize=12, color='white', pad=15)
          
          # 3. ISP distribution (top 10)
          ax3 = fig.add_subplot(gs[1, 2])
          top_isps = df['isp'].value_counts().head(10)
          ax3.bar(range(len(top_isps)), top_isps.values, color='#ff6b6b', alpha=0.8)
          ax3.set_xticks(range(len(top_isps)))
          ax3.set_xticklabels([isp[:15] + '...' if len(isp) > 15 else isp for isp in top_isps.index], 
                             rotation=45, ha='right', fontsize=9)
          ax3.set_ylabel("Count", fontsize=10, color='white')
          ax3.set_title("Top 10 ISPs", fontsize=12, color='white', pad=15)
          ax3.grid(axis='y', alpha=0.3)

          # 4. Anonymity levels
          ax4 = fig.add_subplot(gs[0, 3])
          anonymity_counts = df['anonymity'].value_counts()
          colors_anon = ['#00ff88', '#ffd93d', '#ff6b6b']
          ax4.bar(anonymity_counts.index, anonymity_counts.values, 
                 color=colors_anon[:len(anonymity_counts)], alpha=0.8)
          ax4.set_title("Anonymity Levels", fontsize=12, color='white', pad=15)
          ax4.set_ylabel("Count", fontsize=10, color='white')
          ax4.tick_params(axis='x', rotation=45)
          ax4.grid(axis='y', alpha=0.3)

          # 5. Cities with most proxies
          ax5 = fig.add_subplot(gs[1, 3])
          top_cities = df['city'].value_counts().head(8)
          ax5.bar(range(len(top_cities)), top_cities.values, color='#a8e6cf', alpha=0.8)
          ax5.set_xticks(range(len(top_cities)))
          ax5.set_xticklabels([city[:10] + '...' if len(city) > 10 else city for city in top_cities.index], 
                             rotation=45, ha='right', fontsize=9)
          ax5.set_ylabel("Count", fontsize=10, color='white')
          ax5.set_title("Top 8 Cities", fontsize=12, color='white', pad=15)
          ax5.grid(axis='y', alpha=0.3)

          # 6. Latency distribution (histogram)
          ax6 = fig.add_subplot(gs[2, :2])
          latency_data = df[df['latency'] < 5000]['latency']
          n, bins, patches = ax6.hist(latency_data, bins=50, color='#00ff88', alpha=0.7, edgecolor='none')
          
          # Color code the bars
          for i, patch in enumerate(patches):
              if bins[i] < 500:
                  patch.set_facecolor('#00ff00')
              elif bins[i] < 1000:
                  patch.set_facecolor('#7fff00') 
              elif bins[i] < 1500:
                  patch.set_facecolor('#ffff00')
              elif bins[i] < 2000:
                  patch.set_facecolor('#ffa500')
              elif bins[i] < 3000:
                  patch.set_facecolor('#ff6347')
              else:
                  patch.set_facecolor('#ff0000')
          
          ax6.set_xlabel("Latency (ms)", fontsize=12, color='white')
          ax6.set_ylabel("Count", fontsize=12, color='white')
          ax6.set_title(f"Latency Distribution (Avg: {latency_data.mean():.0f}ms, Median: {latency_data.median():.0f}ms)", 
                       fontsize=14, color='white', pad=20)
          ax6.grid(axis='y', alpha=0.3)
          
          # Add reference lines
          ax6.axvline(1000, color='white', linestyle='--', alpha=0.5, linewidth=1)
          ax6.axvline(2000, color='white', linestyle='--', alpha=0.5, linewidth=1)
          ax6.text(500, ax6.get_ylim()[1] * 0.9, 'Fast', ha='center', fontsize=11, color='white', alpha=0.7)
          ax6.text(1500, ax6.get_ylim()[1] * 0.9, 'Medium', ha='center', fontsize=11, color='white', alpha=0.7)
          ax6.text(3500, ax6.get_ylim()[1] * 0.9, 'Slow', ha='center', fontsize=11, color='white', alpha=0.7)

          # 7. Performance summary (enhanced)
          ax7 = fig.add_subplot(gs[2, 2:])
          performance_data = [fast_count, medium_count, slow_count]
          performance_labels = ['Fast\n(<1s)', 'Medium\n(1-2s)', 'Slow\n(>2s)']
          performance_colors = ['#00ff00', '#ffa500', '#ff0000']
          
          bars = ax7.bar(performance_labels, performance_data, 
                        color=performance_colors, alpha=0.8, width=0.6)
          ax7.set_title("Performance Distribution", fontsize=14, color='white', pad=20)
          ax7.set_ylabel("Number of Proxies", fontsize=12, color='white')
          ax7.grid(axis='y', alpha=0.3)
          
          # Add percentage labels
          total = sum(performance_data)
          for i, (bar, count) in enumerate(zip(bars, performance_data)):
              height = bar.get_height()
              percentage = (count / total) * 100
              ax7.text(bar.get_x() + bar.get_width()/2., height + max(performance_data) * 0.01,
                      f'{count:,}\n({percentage:.1f}%)', 
                      ha='center', va='bottom', fontsize=11, color='white', weight='bold')

          plt.tight_layout()
          plt.savefig("public/analytics.png", dpi=150, bbox_inches='tight', 
                     facecolor='#0a0a0a', edgecolor='none')
          plt.close()
          print(f"✅ Comprehensive analytics chart saved")

          # Generate additional specialized charts
          
          # Geographic distribution heatmap data
          print("🌍 Generating geographic analysis...")
          
          # Save comprehensive summary data
          summary_data = {
              "timestamp": datetime.now().isoformat(),
              "collection_summary": {
                  "total_proxies": len(all_proxies),
                  "unique_ips": len(unique_ips),
                  "files_processed": len(processed_files),
                  "files_failed": len(failed_files),
                  "success_rate": len(processed_files) / (len(processed_files) + len(failed_files)) * 100
              },
              "geographic_data": {
                  "countries": int(df['country'].nunique()),
                  "cities": int(df['city'].nunique()),
                  "top_countries": top_countries.head(10).to_dict(),
                  "top_cities": df['city'].value_counts().head(10).to_dict()
              },
              "performance_metrics": {
                  "average_latency": float(df[df['latency'] < 9999]['latency'].mean()),
                  "median_latency": float(df[df['latency'] < 9999]['latency'].median()),
                  "fast_proxies": int(fast_count),
                  "medium_proxies": int(medium_count), 
                  "slow_proxies": int(slow_count),
                  "performance_distribution": {
                      "fast_percentage": round(fast_count/len(all_proxies)*100, 2),
                      "medium_percentage": round(medium_count/len(all_proxies)*100, 2),
                      "slow_percentage": round(slow_count/len(all_proxies)*100, 2)
                  }
              },
              "technical_data": {
                  "protocol_distribution": protocol_counts.to_dict(),
                  "anonymity_distribution": df['anonymity'].value_counts().to_dict(),
                  "top_isps": df['isp'].value_counts().head(10).to_dict()
              },
              "data_sources": {
                  "processed_files": processed_files,
                  "failed_files": failed_files
              }
          }
          
          # Save all data files
          with open("public/summary.json", "w") as f:
              json.dump(summary_data, f, indent=2)
          
          # Export raw data for external analysis
          df_export = df.copy()
          df_export.to_json("public/raw_data.json", orient="records", indent=2)
          df_export[['ip', 'port', 'country', 'city', 'latency', 'protocol', 'anonymity']].to_csv("public/proxies.csv", index=False)

          print(f"\n🎉 GENERATION COMPLETE!")
          print(f"✅ Generated comprehensive map with {len(all_proxies):,} total proxies")
          print(f"🌍 Covering {df['country'].nunique()} countries and {df['city'].nunique()} cities")
          print(f"📈 Average latency: {df[df['latency'] < 9999]['latency'].mean():.0f}ms")
          print(f"📊 Success rate: {len(processed_files)/(len(processed_files) + len(failed_files))*100:.1f}%")
          print(f"🗺️ Map: public/index.html")
          print(f"📈 Analytics: public/analytics.png")
          print(f"📄 Summary: public/summary.json")
          print(f"💾 Raw data: public/raw_data.json, public/proxies.csv")
          
          EOF

      - name: Create .nojekyll file
        run: echo "" > public/.nojekyll

      - name: Upload Pages artifact
        uses: actions/upload-pages-artifact@v3
        with:
          path: './public'

      - name: Deploy to GitHub Pages
        id: deployment
        uses: actions/deploy-pages@v4
