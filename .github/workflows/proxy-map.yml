name: Generate Enhanced Proxy Map - Improved

on:
  schedule:
    - cron: '0 */6 * * *'  # Run every 6 hours
  workflow_dispatch:  # Allow manual triggering

permissions:
  contents: write
  pages: write
  id-token: write

jobs:
  build-and-deploy:
    runs-on: ubuntu-22.04
    timeout-minutes: 30  # Add timeout to prevent hanging
    
    environment:
      name: github-pages
      url: ${{ steps.deployment.outputs.page_url }}
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          pip install --upgrade pip
          pip install folium requests matplotlib pandas seaborn

      - name: Generate proxy map with improved handling
        run: |
          python3 << 'EOF'
          import requests, folium, pandas as pd, matplotlib.pyplot as plt
          from folium.plugins import Fullscreen, MiniMap, FloatImage, MarkerCluster
          import os, re, json, html, gc
          from datetime import datetime
          import time
          from concurrent.futures import ThreadPoolExecutor, as_completed
          import warnings
          import numpy as np
          warnings.filterwarnings('ignore')

          # Configuration
          CHUNK_SIZE = 5000  # Process proxies in chunks
          MAX_PROXIES_PER_FILE = 10000  # Limit per file to manage memory
          MAX_TOTAL_PROXIES = 50000  # Overall limit
          GITHUB_API_BASE = "https://api.github.com"
          
          def check_rate_limit(response):
              """Check and log GitHub API rate limit"""
              if hasattr(response, 'headers'):
                  remaining = response.headers.get('X-RateLimit-Remaining', 'Unknown')
                  reset_time = response.headers.get('X-RateLimit-Reset', 'Unknown')
                  
                  if remaining != 'Unknown':
                      remaining = int(remaining)
                      if remaining < 10:
                          print(f"‚ö†Ô∏è  Low API rate limit: {remaining} requests remaining")
                          if remaining < 5:
                              wait_time = 60
                              print(f"   Waiting {wait_time}s to avoid rate limit...")
                              time.sleep(wait_time)
                  
                  return remaining
              return None

          def sanitize_html(text):
              """Escape HTML special characters to prevent injection"""
              if text is None:
                  return 'N/A'
              return html.escape(str(text))

          def is_valid_coordinate(lat, lon):
              """Enhanced coordinate validation"""
              if lat is None or lon is None:
                  return False
              try:
                  lat, lon = float(lat), float(lon)
                  # Check for common invalid coordinates
                  if lat == 0 and lon == 0:  # Null Island
                      return False
                  if abs(lat) > 90 or abs(lon) > 180:
                      return False
                  return True
              except (ValueError, TypeError):
                  return False

          def validate_proxy_data(proxy):
              """Validate that proxy has minimum required fields"""
              if not isinstance(proxy, dict):
                  return False
              
              # Check for IP
              ip = proxy.get('ip') or proxy.get('host') or proxy.get('address')
              if not ip:
                  return False
              
              # Check for location
              location = proxy.get('location', {})
              if not location and 'lat' in proxy and 'lon' in proxy:
                  location = proxy
              
              lat = location.get('lat')
              lon = location.get('lon')
              
              return is_valid_coordinate(lat, lon)

          def fetch_with_retry(url, max_retries=3, timeout=60):
              """Fetch URL with retry logic and rate limit handling"""
              for attempt in range(max_retries):
                  try:
                      print(f"  Attempt {attempt + 1}/{max_retries} for {url}")
                      response = requests.get(url, timeout=timeout, headers={
                          'User-Agent': 'ProxyMapBot/1.0',
                          'Accept': 'application/json'
                      })
                      
                      # Check rate limit
                      check_rate_limit(response)
                      
                      if response.status_code == 200:
                          return response.json()
                      elif response.status_code == 429:
                          wait_time = min(60, 2 ** attempt * 5)
                          print(f"  Rate limited (429), waiting {wait_time}s...")
                          time.sleep(wait_time)
                      elif response.status_code == 404:
                          print(f"  File not found (404)")
                          return None
                      else:
                          print(f"  HTTP {response.status_code}")
                          
                  except requests.exceptions.Timeout:
                      print(f"  Timeout on attempt {attempt + 1}")
                  except requests.exceptions.ConnectionError:
                      print(f"  Connection error on attempt {attempt + 1}")
                  except Exception as e:
                      print(f"  Error on attempt {attempt + 1}: {type(e).__name__}: {e}")
                  
                  if attempt < max_retries - 1:
                      time.sleep(min(30, 2 ** attempt))
              
              return None

          def find_all_proxy_files():
              """Find ALL proxy files from the GitHub repo with error handling"""
              try:
                  api_url = f"{GITHUB_API_BASE}/repos/delldevmann/proxy-scraper/contents/results"
                  response = requests.get(api_url, timeout=30, headers={
                      'User-Agent': 'ProxyMapBot/1.0',
                      'Accept': 'application/vnd.github.v3+json'
                  })
                  
                  check_rate_limit(response)
                  
                  if response.status_code != 200:
                      print(f"GitHub API returned status {response.status_code}")
                      return []
                      
                  files = response.json()
                  print(f"Found {len(files)} files in results directory")
                  
                  proxy_files = []
                  priority_keywords = ['summary', 'verified', 'working']  # Prioritize these
                  
                  for file in files:
                      if not isinstance(file, dict):
                          continue
                          
                      filename = file.get('name', '')
                      download_url = file.get('download_url')
                      
                      if not download_url:
                          continue
                      
                      # Look for proxy JSON files
                      if (filename.endswith('.json') and 
                          any(keyword in filename.lower() for keyword in [
                              'all_proxies', 'working_proxies', 'verified_proxies', 
                              'socks4', 'socks5', 'http', 'https', 'proxy'
                          ])):
                          
                          # Determine priority
                          priority = 1 if any(kw in filename.lower() for kw in priority_keywords) else 2
                          
                          print(f"Found proxy file: {filename} (priority: {priority})")
                          proxy_files.append((download_url, filename, priority))
                  
                  # Sort by priority (1 = high, 2 = low)
                  proxy_files.sort(key=lambda x: x[2])
                  
                  print(f"Total proxy files found: {len(proxy_files)}")
                  return [(url, name) for url, name, _ in proxy_files]
                      
              except Exception as e:
                  print(f"Error finding proxy files: {type(e).__name__}: {e}")
                  return []

          def process_summary_data(summary_data):
              """Process the summary JSON with robust schema handling"""
              proxies = []
              
              if not summary_data or not isinstance(summary_data, dict):
                  return proxies
              
              print(f"Processing summary data with keys: {list(summary_data.keys())}")
              
              # Process each protocol type
              for protocol_type, protocol_data in summary_data.items():
                  if not isinstance(protocol_data, dict):
                      continue
                  
                  print(f"\n=== Processing {protocol_type.upper()} ===")
                  print(f"  Total scraped: {protocol_data.get('total_scraped', 0):,}")
                  print(f"  Verified proxies: {protocol_data.get('verified_proxies', 0)}")
                  print(f"  Success rate: {protocol_data.get('verification_success_rate', 'Unknown')}")
                  
                  # Get sample proxies
                  sample_proxies = protocol_data.get('sample_proxies', {})
                  print(f"  Sample proxies available: {len(sample_proxies)}")
                  
                  processed_count = 0
                  for proxy_key, proxy_data in sample_proxies.items():
                      if processed_count >= MAX_PROXIES_PER_FILE:
                          print(f"  Reached limit of {MAX_PROXIES_PER_FILE} proxies for {protocol_type}")
                          break
                      
                      try:
                          if not validate_proxy_data(proxy_data):
                              continue
                          
                          location = proxy_data.get('location', {})
                          lat = float(location.get('lat'))
                          lon = float(location.get('lon'))
                          
                          proxies.append({
                              "ip": sanitize_html(proxy_data.get('ip', '')),
                              "port": sanitize_html(proxy_key.split(':')[1] if ':' in proxy_key else 'Unknown'),
                              "latency": min(9999, int(proxy_data.get('latency_ms', 9999))),
                              "anonymity": sanitize_html(proxy_data.get('anonymity', 'Unknown')),
                              "lat": lat,
                              "lon": lon,
                              "city": sanitize_html(location.get('city', 'Unknown')),
                              "country": sanitize_html(location.get('country', 'Unknown')),
                              "code": sanitize_html(location.get('countryCode', 'xx').lower()),
                              "isp": sanitize_html(location.get('isp', 'N/A')),
                              "source": f"summary_{protocol_type}",
                              "protocol": sanitize_html(proxy_data.get('type', protocol_type)),
                              "last_checked": sanitize_html(proxy_data.get('last_checked', 'Unknown')),
                              "region": sanitize_html(location.get('region', 'Unknown')),
                              "org": sanitize_html(location.get('org', 'N/A')),
                              "as": sanitize_html(location.get('as', 'N/A')),
                              "timezone": sanitize_html(location.get('timezone', 'Unknown'))
                          })
                          processed_count += 1
                          
                      except Exception as e:
                          continue
              
              print(f"‚úÖ Extracted {len(proxies)} valid proxies from summary")
              return proxies

          def process_proxy_chunk(proxy_chunk, filename):
              """Process a chunk of proxies"""
              proxies = []
              
              for proxy in proxy_chunk:
                  if not validate_proxy_data(proxy):
                      continue
                  
                  try:
                      # Extract IP
                      proxy_ip = proxy.get('ip') or proxy.get('host') or proxy.get('address')
                      
                      # Extract location
                      location = proxy.get('location', {})
                      if not location and 'lat' in proxy and 'lon' in proxy:
                          location = proxy
                      
                      lat = float(location.get('lat'))
                      lon = float(location.get('lon'))
                      
                      proxies.append({
                          "ip": sanitize_html(proxy_ip),
                          "port": sanitize_html(str(proxy.get('port', 'Unknown'))),
                          "latency": min(9999, int(proxy.get('latency_ms') or proxy.get('latency', 9999))),
                          "anonymity": sanitize_html(proxy.get('anonymity', 'Unknown')),
                          "lat": lat,
                          "lon": lon,
                          "city": sanitize_html(location.get('city', 'Unknown')),
                          "country": sanitize_html(location.get('country', 'Unknown')),
                          "code": sanitize_html(location.get('countryCode', 'xx').lower()),
                          "isp": sanitize_html(location.get('isp', 'N/A')),
                          "source": sanitize_html(filename),
                          "protocol": sanitize_html(proxy.get('type') or proxy.get('protocol', 'Unknown')),
                          "last_checked": sanitize_html(proxy.get('last_checked', 'Unknown'))
                      })
                      
                  except Exception:
                      continue
              
              return proxies

          def process_complete_file_data(file_data, filename):
              """Process complete proxy files with memory-efficient chunking"""
              proxies = []
              
              if not file_data:
                  return proxies
              
              print(f"Processing {filename}: {type(file_data)}")
              
              # Debug: Show structure
              if isinstance(file_data, dict):
                  print(f"  Top-level keys: {list(file_data.keys())[:10]}")
              
              # Extract proxy candidates
              proxy_candidates = []
              
              if isinstance(file_data, list):
                  # Direct list of proxies
                  proxy_candidates = file_data
              elif isinstance(file_data, dict):
                  # Check if the dict IS the proxy data (all keys are IPs)
                  sample_keys = list(file_data.keys())[:5]
                  if sample_keys and all(':' in str(k) or '.' in str(k) for k in sample_keys):
                      # This is a dict where keys are IP:port
                      print(f"  Detected IP:port dictionary format")
                      for proxy_key, proxy_data in list(file_data.items())[:MAX_PROXIES_PER_FILE]:
                          if isinstance(proxy_data, dict):
                              # Add the IP from the key if not in the data
                              if 'ip' not in proxy_data and ':' in proxy_key:
                                  proxy_data['ip'] = proxy_key.split(':')[0]
                                  proxy_data['port'] = proxy_key.split(':')[1]
                              proxy_candidates.append(proxy_data)
                  else:
                      # Handle various nested structures
                      for key, value in file_data.items():
                          if isinstance(value, list):
                              proxy_candidates.extend(value[:MAX_PROXIES_PER_FILE - len(proxy_candidates)])
                          elif isinstance(value, dict):
                              # Check for nested proxy structures
                              if 'sample_proxies' in value:
                                  sample_proxies = value['sample_proxies']
                                  if isinstance(sample_proxies, dict):
                                      for pk, pv in list(sample_proxies.items())[:MAX_PROXIES_PER_FILE - len(proxy_candidates)]:
                                          if isinstance(pv, dict):
                                              if 'ip' not in pv and ':' in pk:
                                                  pv['ip'] = pk.split(':')[0]
                                                  pv['port'] = pk.split(':')[1]
                                              proxy_candidates.append(pv)
                              elif any(field in value for field in ['ip', 'host', 'address', 'location']):
                                  proxy_candidates.append(value)
                              else:
                                  # Maybe it's a protocol section
                                  for nested_key, nested_value in value.items():
                                      if isinstance(nested_value, dict) and any(field in nested_value for field in ['ip', 'host', 'location']):
                                          proxy_candidates.append(nested_value)
                                      elif isinstance(nested_value, list):
                                          proxy_candidates.extend(nested_value[:MAX_PROXIES_PER_FILE - len(proxy_candidates)])
              
              # Limit candidates
              proxy_candidates = proxy_candidates[:MAX_PROXIES_PER_FILE]
              print(f"  Found {len(proxy_candidates)} proxy candidates")
              
              if not proxy_candidates:
                  print(f"  No valid proxy structure found in {filename}")
                  return proxies
              
              # Process in chunks
              for i in range(0, len(proxy_candidates), CHUNK_SIZE):
                  chunk = proxy_candidates[i:i+CHUNK_SIZE]
                  chunk_proxies = process_proxy_chunk(chunk, filename)
                  proxies.extend(chunk_proxies)
                  
                  if i % (CHUNK_SIZE * 2) == 0 and i > 0:
                      print(f"    Processed: {i}/{len(proxy_candidates)}")
                      gc.collect()  # Force garbage collection
              
              print(f"‚úÖ Extracted {len(proxies)} valid proxies from {filename}")
              return proxies

          # Initialize data collection
          all_proxies = []
          data_sources = []
          
          print("=== COMPREHENSIVE PROXY DATA COLLECTION ===")
          print(f"Starting at: {datetime.now()}")
          print(f"Limits: {MAX_PROXIES_PER_FILE:,} per file, {MAX_TOTAL_PROXIES:,} total")
          
          # 1. Fetch summary data (highest priority)
          print("\n1. Fetching SUMMARY data (verified proxies)...")
          summary_url = 'https://raw.githubusercontent.com/delldevmann/proxy-scraper/main/results/summary_latest.json'
          summary_data = fetch_with_retry(summary_url)
          
          if summary_data:
              summary_proxies = process_summary_data(summary_data)
              all_proxies.extend(summary_proxies)
              data_sources.append(f"Summary (verified): {len(summary_proxies)} proxies")
              
              # Show protocol breakdown
              protocol_counts = {}
              for proxy in summary_proxies:
                  protocol = proxy.get('protocol', 'Unknown')
                  protocol_counts[protocol] = protocol_counts.get(protocol, 0) + 1
              print(f"  Protocol breakdown: {protocol_counts}")
          else:
              print("‚ö†Ô∏è  Failed to fetch summary data - will continue with other sources")
          
          # 2. Fetch additional complete datasets
          if len(all_proxies) < MAX_TOTAL_PROXIES:
              print("\n2. Finding additional proxy files...")
              proxy_files = find_all_proxy_files()
              
              if proxy_files:
                  # Limit files to process based on remaining capacity
                  remaining_capacity = MAX_TOTAL_PROXIES - len(all_proxies)
                  max_files = min(10, max(1, remaining_capacity // 5000))
                  
                  files_to_process = proxy_files[:max_files]
                  print(f"Processing {len(files_to_process)} files (based on capacity)...")
                  
                  def fetch_and_process_file(file_info):
                      url, filename = file_info
                      
                      # Skip if we've reached capacity
                      if len(all_proxies) >= MAX_TOTAL_PROXIES:
                          return []
                      
                      print(f"\nFetching {filename}...")
                      
                      file_data = fetch_with_retry(url, max_retries=2, timeout=90)
                      if file_data:
                          return process_complete_file_data(file_data, filename)
                      return []
                  
                  # Process files with controlled concurrency
                  with ThreadPoolExecutor(max_workers=3) as executor:
                      future_to_file = {executor.submit(fetch_and_process_file, file_info): file_info 
                                       for file_info in files_to_process}
                      
                      for future in as_completed(future_to_file):
                          file_info = future_to_file[future]
                          try:
                              file_proxies = future.result()
                              if file_proxies:
                                  # Remove duplicates based on IP
                                  existing_ips = {p["ip"] for p in all_proxies}
                                  new_proxies = [p for p in file_proxies if p["ip"] not in existing_ips]
                                  
                                  # Check capacity
                                  remaining = MAX_TOTAL_PROXIES - len(all_proxies)
                                  if remaining > 0:
                                      new_proxies = new_proxies[:remaining]
                                      
                                      if new_proxies:
                                          all_proxies.extend(new_proxies)
                                          data_sources.append(f"{file_info[1]}: +{len(new_proxies)} additional")
                                          print(f"‚úÖ Added {len(new_proxies)} new proxies from {file_info[1]}")
                                  else:
                                      print(f"‚ö†Ô∏è  Reached capacity limit of {MAX_TOTAL_PROXIES} proxies")
                                      break
                          except Exception as e:
                              print(f"‚ùå Error processing {file_info[1]}: {type(e).__name__}: {e}")

          print(f"\n=== FINAL RESULTS ===")
          print(f"üéâ Total unique proxies collected: {len(all_proxies):,}")
          print(f"üìä Data sources used: {len(data_sources)}")
          
          for i, source in enumerate(data_sources, 1):
              print(f"  {i}. {source}")

          # Check if we have any data
          if not all_proxies:
              print("\n‚ùå No proxy data found!")
              print("Creating placeholder page...")
              
              # Create a minimal HTML page explaining the issue
              os.makedirs("public", exist_ok=True)
              
              with open("public/index.html", "w") as f:
                  f.write("""
                  <!DOCTYPE html>
                  <html>
                  <head>
                      <title>Proxy Map - No Data Available</title>
                      <style>
                          body { font-family: Arial, sans-serif; text-align: center; padding: 50px; }
                          h1 { color: #e74c3c; }
                          .info { background: #f8f9fa; padding: 20px; border-radius: 10px; max-width: 600px; margin: 0 auto; }
                      </style>
                  </head>
                  <body>
                      <h1>‚ö†Ô∏è No Proxy Data Available</h1>
                      <div class="info">
                          <p>The proxy data could not be fetched at this time.</p>
                          <p>Possible reasons:</p>
                          <ul style="text-align: left; display: inline-block;">
                              <li>The data source is temporarily unavailable</li>
                              <li>API rate limits have been exceeded</li>
                              <li>Network connectivity issues</li>
                          </ul>
                          <p>The system will retry in the next scheduled run.</p>
                          <p><small>Last attempt: """ + datetime.now().strftime('%Y-%m-%d %H:%M UTC') + """</small></p>
                      </div>
                  </body>
                  </html>
                  """)
              
              print("‚úÖ Created placeholder page")
              exit(0)  # Exit successfully to allow deployment

          # Analyze the data
          df = pd.DataFrame(all_proxies)
          
          # Clean up latency values
          df['latency'] = pd.to_numeric(df['latency'], errors='coerce').fillna(9999)
          df['latency'] = df['latency'].clip(upper=9999)
          
          # Calculate average latency safely
          valid_latencies = df[df['latency'] < 9999]['latency']
          avg_latency = valid_latencies.mean() if len(valid_latencies) > 0 else 9999
          
          print(f"\nüìà ANALYSIS:")
          print(f"  üåç Countries represented: {df['country'].nunique()}")
          print(f"  ‚ö° Average latency: {avg_latency:.0f}ms")
          print(f"  üöÄ Fast proxies (<1000ms): {len(df[df['latency'] < 1000])}")
          print(f"  üü° Medium proxies (1000-2000ms): {len(df[(df['latency'] >= 1000) & (df['latency'] < 2000)])}")
          print(f"  üî¥ Slow proxies (>2000ms): {len(df[df['latency'] >= 2000])}")
          
          # Show top countries
          top_countries = df['country'].value_counts().head(10)
          print(f"  üèÜ Top countries:")
          for country, count in top_countries.items():
              print(f"    {country}: {count}")

          # Generate the enhanced map
          print(f"\nüó∫Ô∏è Generating interactive map...")
          
          # Calculate map center
          lat_center, lon_center = df["lat"].mean(), df["lon"].mean()

          # Create map with better styling
          m = folium.Map(
              location=[lat_center, lon_center], 
              zoom_start=2, 
              tiles="CartoDB dark_matter",
              control_scale=True,
              prefer_canvas=True,  # Better performance
              max_zoom=18
          )
          
          # Add map plugins
          Fullscreen().add_to(m)
          MiniMap(toggle_display=True, position='bottomright').add_to(m)
          
          # Use marker clustering for performance
          marker_cluster = MarkerCluster(
              name="Proxy Locations",
              overlay=True,
              control=True,
              options={
                  'maxClusterRadius': 50,
                  'disableClusteringAtZoom': 10,
                  'spiderfyOnMaxZoom': True,
                  'showCoverageOnHover': False
              }
          ).add_to(m)

          # Add markers (limit if too many)
          markers_to_add = min(len(df), 20000)  # Limit markers for performance
          if markers_to_add < len(df):
              print(f"‚ö†Ô∏è  Limiting map markers to {markers_to_add:,} for performance")
              df_sample = df.sample(n=markers_to_add, random_state=42)
          else:
              df_sample = df
          
          print(f"Adding {len(df_sample):,} markers to map...")
          
          for idx, (_, row) in enumerate(df_sample.iterrows()):
              if idx % 1000 == 0 and idx > 0:
                  print(f"  Progress: {idx}/{len(df_sample)} markers")
              
              # Color coding based on latency
              if row["latency"] < 1000:
                  color = '#00ff00'  # Bright green for fast
                  size = 8
              elif row["latency"] < 2000:
                  color = '#ffa500'  # Orange for medium
                  size = 6
              else:
                  color = '#ff0000'  # Red for slow
                  size = 4
              
              # Country flag URL (with fallback)
              flag_url = f"https://flagcdn.com/24x18/{row['code']}.png"
              
              # Enhanced popup with all available data
              popup_content = f"""
              <div style='font-family: Arial, sans-serif; font-size: 12px; width: 320px; padding: 5px;'>
                <div style='background: linear-gradient(90deg, #667eea 0%, #764ba2 100%); color: white; padding: 8px; margin: -5px -5px 10px -5px; border-radius: 4px;'>
                  <img src='{flag_url}' style='vertical-align: middle; margin-right: 8px;' onerror="this.style.display='none'"> 
                  <strong style='font-size: 14px;'>{row['country']}</strong>
                </div>
                
                <table style='width: 100%; font-size: 11px;'>
                  <tr><td><strong>üåê IP:Port</strong></td><td>{row['ip']}:{row['port']}</td></tr>
                  <tr><td><strong>üìç Location</strong></td><td>{row['city']}, {row.get('region', 'N/A')}</td></tr>
                  <tr><td><strong>üè¢ ISP</strong></td><td>{row['isp']}</td></tr>
                  <tr><td><strong>‚ö° Latency</strong></td><td>{row['latency']} ms</td></tr>
                  <tr><td><strong>üîí Anonymity</strong></td><td>{row['anonymity']}</td></tr>
                  <tr><td><strong>üì° Protocol</strong></td><td>{row['protocol'].upper()}</td></tr>
                  <tr><td><strong>üìä Source</strong></td><td>{row['source']}</td></tr>
                  <tr><td><strong>üïí Last Check</strong></td><td>{row.get('last_checked', 'Unknown')}</td></tr>
                </table>
              </div>
              """
              
              folium.CircleMarker(
                  location=[row["lat"], row["lon"]],
                  radius=size,
                  color='white',
                  weight=1,
                  fill=True,
                  fillColor=color,
                  fillOpacity=0.8,
                  popup=folium.Popup(popup_content, max_width=350),
                  tooltip=f"{row['ip']} | {row['country']} | {row['latency']}ms"
              ).add_to(marker_cluster)

          # Create enhanced visualizations
          os.makedirs("public", exist_ok=True)
          
          # Country distribution chart
          plt.style.use('dark_background')
          fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
          
          # Top countries bar chart
          top_countries = df["country"].value_counts().nlargest(12)
          colors = plt.cm.viridis(np.linspace(0, 1, len(top_countries)))
          
          bars = ax1.barh(range(len(top_countries)), top_countries.values, color=colors)
          ax1.set_yticks(range(len(top_countries)))
          ax1.set_yticklabels(top_countries.index)
          ax1.set_xlabel("Number of Proxies", fontweight='bold')
          ax1.set_title(f"Top Countries ({len(all_proxies):,} total proxies)", fontweight='bold', pad=20)
          ax1.grid(axis='x', alpha=0.3)
          
          # Add value labels
          for i, (country, count) in enumerate(top_countries.items()):
              ax1.text(count + max(top_countries.values()) * 0.01, i, str(count), 
                      va='center', ha='left', fontweight='bold')
          
          # Protocol distribution pie chart
          protocol_counts = df['protocol'].value_counts()
          colors_pie = plt.cm.Set3(np.linspace(0, 1, len(protocol_counts)))
          ax2.pie(protocol_counts.values, labels=protocol_counts.index, autopct='%1.1f%%', 
                  colors=colors_pie)
          ax2.set_title("Protocol Distribution", fontweight='bold', pad=20)
          
          plt.tight_layout()
          plt.savefig("public/analysis_charts.png", dpi=150, bbox_inches='tight', 
                     facecolor='#2e2e2e', edgecolor='none')
          plt.close()

          # Enhanced statistics panel
          fast_count = len(df[df['latency'] < 1000])
          medium_count = len(df[(df['latency'] >= 1000) & (df['latency'] < 2000)])
          slow_count = len(df[df['latency'] >= 2000])
          
          stats_html = f"""
          <div style="position: fixed; 
                      top: 10px; left: 10px; width: 320px; 
                      background: linear-gradient(135deg, rgba(0,0,0,0.9) 0%, rgba(30,30,30,0.95) 100%); 
                      border: 2px solid #667eea; border-radius: 12px;
                      z-index: 9999; font-size: 13px; padding: 16px;
                      box-shadow: 0 8px 32px rgba(0,0,0,0.3);
                      color: white; font-family: 'Segoe UI', Arial, sans-serif;">
          
          <div style="text-align: center; margin-bottom: 12px;">
            <h3 style="margin: 0; color: #667eea; font-size: 18px;">üó∫Ô∏è Proxy Network Map</h3>
            <div style="font-size: 11px; color: #aaa; margin-top: 4px;">
              Real-time Global Proxy Intelligence
            </div>
          </div>
          
          <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 8px; margin-bottom: 12px;">
            <div style="background: rgba(102, 126, 234, 0.2); padding: 8px; border-radius: 6px; text-align: center;">
              <div style="font-size: 18px; font-weight: bold; color: #667eea;">{len(all_proxies):,}</div>
              <div style="font-size: 10px; color: #ccc;">Total Proxies</div>
            </div>
            <div style="background: rgba(118, 75, 162, 0.2); padding: 8px; border-radius: 6px; text-align: center;">
              <div style="font-size: 18px; font-weight: bold; color: #764ba2;">{df['country'].nunique()}</div>
              <div style="font-size: 10px; color: #ccc;">Countries</div>
            </div>
          </div>
          
          <div style="margin-bottom: 12px;">
            <div style="display: flex; justify-content: space-between; align-items: center; margin-bottom: 4px;">
              <span style="font-size: 12px;">‚ö° Avg Latency:</span>
              <span style="font-weight: bold; color: #ffa500;">{avg_latency:.0f}ms</span>
            </div>
            <div style="display: flex; justify-content: space-between; align-items: center; margin-bottom: 4px;">
              <span style="font-size: 12px;">üìä Data Sources:</span>
              <span style="font-weight: bold; color: #64ffda;">{len(data_sources)}</span>
            </div>
            <div style="display: flex; justify-content: space-between; align-items: center;">
              <span style="font-size: 12px;">üïí Last Update:</span>
              <span style="font-weight: bold; color: #ff6b6b;">{datetime.now().strftime('%H:%M UTC')}</span>
            </div>
          </div>
          
          <div style="background: rgba(255,255,255,0.1); padding: 10px; border-radius: 6px; margin-bottom: 12px;">
            <div style="font-size: 11px; color: #ccc; margin-bottom: 6px; text-align: center;">Performance Distribution</div>
            <div style="display: flex; justify-content: space-between; font-size: 11px;">
              <div style="text-align: center;">
                <div style="color: #00ff00; font-weight: bold;">{fast_count}</div>
                <div style="color: #aaa;">Fast</div>
              </div>
              <div style="text-align: center;">
                <div style="color: #ffa500; font-weight: bold;">{medium_count}</div>
                <div style="color: #aaa;">Medium</div>
              </div>
              <div style="text-align: center;">
                <div style="color: #ff0000; font-weight: bold;">{slow_count}</div>
                <div style="color: #aaa;">Slow</div>
              </div>
            </div>
          </div>
          
          <div style="font-size: 10px; color: #999; max-height: 80px; overflow-y: auto;">
            <strong style="color: #667eea;">Data Sources:</strong><br>
            {'<br>'.join([f"‚Ä¢ {source}" for source in data_sources[:8]])}
            {f'<br>‚Ä¢ ...and {len(data_sources)-8} more sources' if len(data_sources) > 8 else ''}
          </div>
          </div>
          """
          
          # Enhanced legend
          legend_html = """
          <div style="position: fixed; 
                      top: 10px; right: 10px; width: 180px; 
                      background: linear-gradient(135deg, rgba(0,0,0,0.9) 0%, rgba(30,30,30,0.95) 100%); 
                      border: 2px solid #764ba2; border-radius: 12px;
                      z-index: 9999; font-size: 13px; padding: 16px;
                      box-shadow: 0 8px 32px rgba(0,0,0,0.3);
                      color: white; font-family: 'Segoe UI', Arial, sans-serif;">
          
          <h4 style="margin: 0 0 12px 0; color: #764ba2; text-align: center;">üö¶ Latency Legend</h4>
          
          <div style="margin-bottom: 8px;">
            <div style="display: flex; align-items: center; margin: 6px 0;">
              <div style="width: 16px; height: 16px; background: #00ff00; border-radius: 50%; margin-right: 10px; border: 1px solid white;"></div>
              <span style="font-weight: 500;">< 1000ms (Excellent)</span>
            </div>
            <div style="display: flex; align-items: center; margin: 6px 0;">
              <div style="width: 14px; height: 14px; background: #ffa500; border-radius: 50%; margin-right: 10px; border: 1px solid white;"></div>
              <span style="font-weight: 500;">1000-2000ms (Good)</span>
            </div>
            <div style="display: flex; align-items: center; margin: 6px 0;">
              <div style="width: 12px; height: 12px; background: #ff0000; border-radius: 50%; margin-right: 10px; border: 1px solid white;"></div>
              <span style="font-weight: 500;">> 2000ms (Slow)</span>
            </div>
          </div>
          
          <div style="border-top: 1px solid #444; padding-top: 8px; font-size: 11px; color: #bbb; text-align: center;">
            üí° Click markers for details<br>
            üîç Zoom to explore regions<br>
            üìä Clusters show proxy density
          </div>
          </div>
          """
          
          # Add file size warning
          map_size_warning = ""
          if len(df_sample) > 10000:
              map_size_warning = f"""
              <div style="position: fixed; 
                          bottom: 10px; left: 10px; 
                          background: rgba(255, 152, 0, 0.9); 
                          color: white; padding: 10px; 
                          border-radius: 6px; font-size: 12px;
                          z-index: 9999;">
                ‚ö†Ô∏è Showing {len(df_sample):,} of {len(df):,} proxies for performance
              </div>
              """
          
          # Add the charts as floating image (with error handling)
          try:
              if os.path.exists("public/analysis_charts.png"):
                  FloatImage("analysis_charts.png", bottom=20, right=20).add_to(m)
          except Exception as e:
              print(f"‚ö†Ô∏è  Could not add charts to map: {e}")
          
          # Add the info panels
          m.get_root().html.add_child(folium.Element(stats_html))
          m.get_root().html.add_child(folium.Element(legend_html))
          if map_size_warning:
              m.get_root().html.add_child(folium.Element(map_size_warning))
          
          # Save the map
          m.save("public/index.html")
          
          # Check output file size
          try:
              output_size = os.path.getsize("public/index.html") / (1024 * 1024)  # MB
              print(f"\nüìÅ Output file size: {output_size:.1f}MB")
              if output_size > 50:
                  print(f"‚ö†Ô∏è  Large output file! Consider reducing proxy count.")
              if output_size > 90:
                  print(f"‚ùå File too large for GitHub Pages (>100MB limit)")
          except Exception as e:
              print(f"Could not check file size: {e}")
          
          # Create a summary JSON for diagnostics
          summary_stats = {
              "generated_at": datetime.now().isoformat(),
              "total_proxies": len(all_proxies),
              "unique_countries": int(df['country'].nunique()),
              "average_latency_ms": float(avg_latency),
              "data_sources": len(data_sources),
              "protocol_distribution": df['protocol'].value_counts().to_dict(),
              "top_countries": df['country'].value_counts().head(20).to_dict(),
              "performance_distribution": {
                  "fast": int(fast_count),
                  "medium": int(medium_count),
                  "slow": int(slow_count)
              },
              "markers_on_map": len(df_sample),
              "output_size_mb": output_size if 'output_size' in locals() else None
          }
          
          with open("public/stats.json", "w") as f:
              json.dump(summary_stats, f, indent=2)
          
          print(f"\nüéâ SUCCESS!")
          print(f"‚úÖ Generated interactive map with {len(df_sample):,} markers")
          print(f"üåç Covering {df['country'].nunique()} countries")
          print(f"üìä From {len(all_proxies):,} total proxies collected")
          print(f"üó∫Ô∏è Map saved to: public/index.html")
          print(f"üìä Charts saved to: public/analysis_charts.png")
          print(f"üìà Stats saved to: public/stats.json")
          
          # Final memory cleanup
          del df, all_proxies
          gc.collect()
          
          EOF

      - name: Check generated files
        run: |
          echo "Generated files:"
          ls -la public/
          if [ -f public/index.html ]; then
            echo "Map size: $(du -h public/index.html | cut -f1)"
          fi
          if [ -f public/stats.json ]; then
            echo "Statistics:"
            cat public/stats.json | head -20
          fi

      - name: Setup Pages
        uses: actions/configure-pages@v4

      - name: Upload artifact
        uses: actions/upload-pages-artifact@v3
        with:
          path: 'public'

      - name: Deploy to GitHub Pages
        id: deployment
        uses: actions/deploy-pages@v4
